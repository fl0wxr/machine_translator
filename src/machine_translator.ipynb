{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6298,"status":"ok","timestamp":1679486546447,"user":{"displayName":"Apostolos Dimoulakis","userId":"17777759558514461466"},"user_tz":-120},"id":"-3TWsyt71K1y","outputId":"4db9c283-0ea9-4b2f-e022-f4ceab1bb0ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":446,"status":"ok","timestamp":1679486551772,"user":{"displayName":"Apostolos Dimoulakis","userId":"17777759558514461466"},"user_tz":-120},"id":"zK_1sww1y8pd","outputId":"f1359f82-94f7-4fb5-df2d-b5e0cfc01a15"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU 0: Tesla T4 (UUID: GPU-6490d090-d8fc-d8a3-89e7-82a5794ead7c)\n"]}],"source":["!nvidia-smi -L"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1679486552184,"user":{"displayName":"Apostolos Dimoulakis","userId":"17777759558514461466"},"user_tz":-120},"id":"ZAf28WYEy8pi","outputId":"2e5c9393-3ce1-4bcb-9f83-3b8cf5b5c7fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed Mar 22 12:02:31 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   70C    P0    29W /  70W |   6563MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":706,"status":"ok","timestamp":1679486560527,"user":{"displayName":"Apostolos Dimoulakis","userId":"17777759558514461466"},"user_tz":-120},"id":"XA27Vb9Ly8pj","outputId":"15efb05b-a186-4179-9825-e4f800faa248"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model name:                      Intel(R) Xeon(R) CPU @ 2.00GHz\n"]}],"source":["!lscpu | grep \"Model name\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1679486561230,"user":{"displayName":"Apostolos Dimoulakis","userId":"17777759558514461466"},"user_tz":-120},"id":"O8GvNOnly8pk","outputId":"8f3ae1e8-69cd-47b5-9b62-4488123f7d28"},"outputs":[{"name":"stdout","output_type":"stream","text":["Python 3.9.16\n"]}],"source":["!python3 --version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":607,"status":"ok","timestamp":1679486561832,"user":{"displayName":"Apostolos Dimoulakis","userId":"17777759558514461466"},"user_tz":-120},"id":"FwciVYxYy8pl","outputId":"186c7839-bc52-43e8-bdc3-a685b080a8b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch                         1.13.1+cu116\n","torchaudio                    0.13.1+cu116\n","torchsummary                  1.5.1\n","torchtext                     0.14.1\n","torchvision                   0.14.1+cu116\n"]}],"source":["!pip list | grep torch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5134,"status":"ok","timestamp":1679486566963,"user":{"displayName":"Apostolos Dimoulakis","userId":"17777759558514461466"},"user_tz":-120},"id":"asKFfz0RxeDR","outputId":"8f23c2e5-89b8-4965-aaef-89a383e69877"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: ing_theme_matplotlib in /usr/local/lib/python3.9/dist-packages (0.1.8)\n","Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.9/dist-packages (from ing_theme_matplotlib) (3.7.1)\n","Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.9/dist-packages (from ing_theme_matplotlib) (1.0.0)\n","Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.9/dist-packages (from ing_theme_matplotlib) (1.22.4)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.9/dist-packages (from jupyter>=1.0->ing_theme_matplotlib) (7.7.1)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.9/dist-packages (from jupyter>=1.0->ing_theme_matplotlib) (5.4.1)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.9/dist-packages (from jupyter>=1.0->ing_theme_matplotlib) (6.5.3)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.9/dist-packages (from jupyter>=1.0->ing_theme_matplotlib) (6.1.0)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.9/dist-packages (from jupyter>=1.0->ing_theme_matplotlib) (5.3.4)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.9/dist-packages (from jupyter>=1.0->ing_theme_matplotlib) (6.5.4)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.3->ing_theme_matplotlib) (4.39.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.3->ing_theme_matplotlib) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.3->ing_theme_matplotlib) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.3->ing_theme_matplotlib) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.3->ing_theme_matplotlib) (23.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.3->ing_theme_matplotlib) (1.0.7)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.3->ing_theme_matplotlib) (5.12.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.3->ing_theme_matplotlib) (3.0.9)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.3->ing_theme_matplotlib) (8.4.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.0.3->ing_theme_matplotlib) (3.15.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.3->ing_theme_matplotlib) (1.16.0)\n","Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter>=1.0->ing_theme_matplotlib) (7.9.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter>=1.0->ing_theme_matplotlib) (6.1.12)\n","Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter>=1.0->ing_theme_matplotlib) (6.2)\n","Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter>=1.0->ing_theme_matplotlib) (5.7.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter>=1.0->ing_theme_matplotlib) (3.0.5)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter>=1.0->ing_theme_matplotlib) (3.6.2)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter>=1.0->ing_theme_matplotlib) (0.2.0)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-console->jupyter>=1.0->ing_theme_matplotlib) (2.0.10)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from jupyter-console->jupyter>=1.0->ing_theme_matplotlib) (2.6.1)\n","Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0->ing_theme_matplotlib) (5.3.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0->ing_theme_matplotlib) (0.7.1)\n","Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0->ing_theme_matplotlib) (5.7.3)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0->ing_theme_matplotlib) (4.11.2)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0->ing_theme_matplotlib) (1.5.0)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0->ing_theme_matplotlib) (0.4)\n","Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0->ing_theme_matplotlib) (3.1.2)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0->ing_theme_matplotlib) (4.9.2)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0->ing_theme_matplotlib) (6.0.0)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0->ing_theme_matplotlib) (1.2.1)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0->ing_theme_matplotlib) (0.8.4)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0->ing_theme_matplotlib) (0.7.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0->ing_theme_matplotlib) (2.1.2)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0->ing_theme_matplotlib) (0.2.2)\n","Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter>=1.0->ing_theme_matplotlib) (0.5.3)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter>=1.0->ing_theme_matplotlib) (0.17.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter>=1.0->ing_theme_matplotlib) (21.3.0)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter>=1.0->ing_theme_matplotlib) (1.8.0)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter>=1.0->ing_theme_matplotlib) (0.16.0)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter>=1.0->ing_theme_matplotlib) (23.2.1)\n","Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter>=1.0->ing_theme_matplotlib) (1.5.6)\n","Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.9/dist-packages (from qtconsole->jupyter>=1.0->ing_theme_matplotlib) (2.3.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0->ing_theme_matplotlib) (0.7.5)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0->ing_theme_matplotlib) (0.2.0)\n","Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0->ing_theme_matplotlib) (0.18.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0->ing_theme_matplotlib) (4.4.2)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0->ing_theme_matplotlib) (63.4.3)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0->ing_theme_matplotlib) (4.8.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter>=1.0->ing_theme_matplotlib) (3.1.1)\n","Requirement already satisfied: notebook-shim>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from nbclassic>=0.4.7->notebook->jupyter>=1.0->ing_theme_matplotlib) (0.2.2)\n","Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.9/dist-packages (from nbclassic>=0.4.7->notebook->jupyter>=1.0->ing_theme_matplotlib) (1.23.6)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat>=5.1->nbconvert->jupyter>=1.0->ing_theme_matplotlib) (4.3.3)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat>=5.1->nbconvert->jupyter>=1.0->ing_theme_matplotlib) (2.16.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter>=1.0->ing_theme_matplotlib) (0.2.6)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.9/dist-packages (from terminado>=0.8.3->notebook->jupyter>=1.0->ing_theme_matplotlib) (0.7.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook->jupyter>=1.0->ing_theme_matplotlib) (21.2.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert->jupyter>=1.0->ing_theme_matplotlib) (2.4)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert->jupyter>=1.0->ing_theme_matplotlib) (0.5.1)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter>=1.0->ing_theme_matplotlib) (0.8.3)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter>=1.0->ing_theme_matplotlib) (22.2.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter>=1.0->ing_theme_matplotlib) (0.19.3)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.9/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter>=1.0->ing_theme_matplotlib) (1.5.1)\n","Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter>=1.0->ing_theme_matplotlib) (3.6.2)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0->ing_theme_matplotlib) (1.15.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.9/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter>=1.0->ing_theme_matplotlib) (3.4)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter>=1.0->ing_theme_matplotlib) (1.3.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0->ing_theme_matplotlib) (2.21)\n"]}],"source":["pip install ing_theme_matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mTuAJLIKy8pm"},"outputs":[],"source":["!gcc ./drive/MyDrive/colab_storage/scripts/preprocessing_tools.c -o ./drive/MyDrive/colab_storage/lib/preprocessing_tools.so -shared"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h46Ysbujy8pm"},"outputs":[],"source":["from copy import deepcopy\n","import random\n","import ctypes\n","import os\n","import functools\n","from time import time\n","import datetime\n","import math\n","from collections import Counter, OrderedDict\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import MaxNLocator\n","from ing_theme_matplotlib import mpl_style\n","\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","import torchtext"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7C1DXUCw2NB"},"outputs":[],"source":["## Load the shared library\n","global lib\n","lib = ctypes.CDLL('./drive/MyDrive/colab_storage/lib/preprocessing_tools.so')\n","\n","\n","def int2str(col_int, vocab, max_steps, trim=True, join=True):\n","    '''\n","    Description:\n","        Converts tokens from integer to string format.\n","    '''\n","\n","    ## trim triggers the sequence's trimming from <eos> until the maximum step\n","    if trim:\n","        trim_condition = lambda str_seq: str_seq == '<eos>'\n","    else:\n","        trim_condition = lambda *args: False\n","\n","    n_examples = col_int.shape[0]\n","\n","    col_str_ = [[vocab.get_itos()[col_int[example_idx][t]] for t in range(max_steps)] for example_idx in range(n_examples)]\n","    col_str__ = [[] for example_idx in range(n_examples)]\n","    for example_idx in range(n_examples):\n","        for step in range(max_steps):\n","            if trim_condition(col_str_[example_idx][step]):\n","                break\n","            col_str__[example_idx].append(col_str_[example_idx][step])\n","\n","    ## join triggers the resulting sequence's concatenation of string-tokens to form the final sentence\n","    if join:\n","        col_str = [' '.join(col_str__[example_idx]) for example_idx in range(n_examples)]\n","    else:\n","        col_str = col_str__\n","\n","    return col_str\n","\n","def ohe2int(p_distr):\n","    '''\n","    Description:\n","        Converts a given distribution to the most likely vocabulary index.\n","    '''\n","\n","    return torch.argmax(p_distr, axis=-1)\n","\n","def ohe2str(p_distr, vocab, max_steps, trim=True, join=True):\n","\n","    return int2str(ohe2int(p_distr), vocab, max_steps, trim, join)\n","\n","def single_gram_to_k_grams(col_int, k):\n","    '''\n","    Description:\n","        Converts word-tokens to multi-word, k-grams.\n","\n","    Inputs:\n","        <col_int>: Type <list[<list[<int>]>]>. The outmost list's length equals n_examples. The innermost list's length equals the corresponding example's sequence length.\n","        <n>: Type <int>.\n","\n","    Outputs:\n","        <col_multitoken_int>: Type <list[<list[<tuple[<int>]>]>]>. The first (in depth) list's length equals n_examples. The second list's length equals the corresponding example's sequence length. The tuple's length equals k.\n","    '''\n","\n","    n_examples = len(col_int)\n","\n","    if k == 1:\n","        col_ngram_int = col_int\n","        return col_ngram_int\n","\n","    col_ngram_int = [[] for i in range(n_examples)]\n","\n","    for i in range(n_examples):\n","    \n","        n_tokens_col_int = len(col_int[i])\n","\n","        n_tokens_col_ngram_int = max(1, n_tokens_col_int - k + 1)\n","        for t_ in range(n_tokens_col_ngram_int):\n","            col_ngram_int[i].append(tuple(col_int[i][t_: t_+k]))\n","\n","    return col_ngram_int\n","\n","def trim_sequence(seq, eos):\n","    '''\n","    Description:\n","        Trims sequences the part that begins with the end of sequence token. Type <d0> is defined to either be <str> or <int>.\n","\n","    Inputs:\n","        <seq>: Type <list[list[<d0>]]>. Length: n_examples.\n","        <eos>: Type <d0>. The end of sequence token.\n","\n","    Outputs:\n","        <updated_seq>: Type <list[list[<d0>]]>. Length: n_examples.\n","    '''\n","\n","    n_examples = len(seq)\n","    for i in range(n_examples):\n","        if eos in seq[i]:\n","            separator = seq[i].index(eos)\n","            seq[i] = seq[i][:separator]\n","\n","    return seq\n","\n","def purger(raw_text):\n","        \"\"\"\n","        Description:\n","            Each line is partitioned with respect to '\\t' the final member of the resulting list is dropped. This successfully gets rid of the useless last segment.\n","\n","        Input:\n","            <raw_text>: Type: <str>.\n","\n","        Output:\n","            <reduced_text>: Type: <str>.\n","        \"\"\"\n","\n","        reduced_text = []\n","        raw_sentence_seq = raw_text.split('\\n')\n","        for instance in raw_sentence_seq:\n","            reduced_text.append('\\t'.join(instance.split('\\t')[:-1]))\n","\n","        reduced_text = '\\n'.join(reduced_text)\n","\n","        return reduced_text\n","\n","def initial_text_preprocess(text):\n","\n","    # Replace non-breaking space with space\n","    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n","\n","    # Insert space between words and punctuation marks\n","    no_space = lambda char, prev_char: char in ',.!?;' and prev_char != ' '\n","    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n","    for i, char in enumerate(text.lower())]\n","\n","    return ''.join(out)\n","\n","def tokenize(text, max_examples=None):\n","\n","    src_str, tgt_str = [], []\n","    for i, line in enumerate(text.split('\\n')):\n","        if max_examples and i > max_examples: break\n","        parts = line.split('\\t')\n","        if len(parts) == 2:\n","            # Skip empty tokens\n","            src_str.append([t for t in f'{parts[0]} <eos>'.split(' ') if t])\n","            tgt_str.append([t for t in f'{parts[1]} <eos>'.split(' ') if t])\n","\n","    return src_str, tgt_str\n","\n","def c_numericalize_str(col_seq, vocab_itos): ## str2int\n","    \"\"\"\n","    Description:\n","        Converts a list of string-token sequences to a list of vocabulary-index-token sequences, while maintaining their order. In other words it replicates the list, whilst replacing the string tokens with index tokens with respect to a vocabulary. It's worth noting that each sequence has a length that potentially varies and is not necessarily fixed.\n","\n","    Inputs:\n","        <col_seq>: Type: <list[<list[<str>]>]>. The collection of sequences of words to be transformed/numericalized.\n","            Warning: It was assumed that for each list-member of <col_seq>, its final string-token-element has to be exactly b'<eos>', otherwise the .so file won't be able to process the input.\n","        <vocab_itos>: Type: <list[<str>]>. The list containing all the vocabularies string-tokens at the position with index equal to vocabulary-index-token. Must include the <unk> character in the beginning.\n","\n","    Returns:\n","        <enc_col>: Type: <list[<list[<int>]>]>. Contains all the members of <col_seq> (\"col\" is for \"collection\") where each string replaced with the vocabularies index.\n","    \"\"\"\n","\n","    ## Prototype\n","    lib.arstr2num.argtypes = \\\n","    [\n","        ctypes.POINTER(ctypes.POINTER(ctypes.c_char_p)), # char ***col_seq\n","        ctypes.c_int, # int col_seq_length\n","        ctypes.POINTER(ctypes.c_char_p), # char **vocab_itos\n","        ctypes.c_int, # vocab_itos_length\n","        ctypes.POINTER(ctypes.POINTER(ctypes.c_int)) # int **enc_col_seq\n","    ]\n","\n","    ## ! Define the arrays' instances and allocate memory for them: Begin\n","\n","    enc_col_seq = [[None for j in range(len(col_seq[i]))] for i in range(len(col_seq))]\n","\n","    col_seq_array = (ctypes.POINTER(ctypes.c_char_p) * len(col_seq))()\n","    for i, seq in enumerate(col_seq):\n","        col_seq_array[i] = (ctypes.c_char_p * len(seq))()\n","\n","    enc_col_seq_array = (ctypes.POINTER(ctypes.c_int) * len(enc_col_seq))()\n","    for i, enc_seq in enumerate(enc_col_seq):\n","        enc_col_seq_array[i] = (ctypes.c_int * len(enc_seq))()\n","\n","    vocab_itos_array = (ctypes.c_char_p * len(vocab_itos))()\n","\n","    ## Allocating memory\n","    for i, seq in enumerate(col_seq):\n","        for j, token_str in enumerate(seq):\n","            col_seq_array[i][j] = token_str.encode()\n","\n","    ## Allocating memory\n","    for i, token_str in enumerate(vocab_itos):\n","        vocab_itos_array[i] = token_str.encode()\n","\n","    ## ! Define the arrays' instances and allocate memory for them: End\n","\n","    ## Update <enc_col_seq_array>\n","    lib.arstr2num(col_seq_array, len(col_seq), vocab_itos_array, len(vocab_itos), enc_col_seq_array)\n","\n","    ## Copy values from <enc_col_seq_array> to <enc_col_seq>\n","    for i in range(len(enc_col_seq)):\n","        for j in range(len(enc_col_seq[i])):\n","            enc_col_seq[i][j] = enc_col_seq_array[i][j]\n","\n","    return enc_col_seq\n","\n","str2int = c_numericalize_str\n","\n","def c_pad_or_trim(enc_col, eos_int, pad_int, t_bound):\n","    \"\"\"\n","    Description:\n","        For each sequence in <enc_col>:\n","        1. If the sequence contains more than <t_bound> tokens, this function removes all the right side tokens with index greater than <t_bound>.\n","        2. If the sequence contains less than <t_bound> tokens, this function adds the integer-encoded padding token to its right side until the sequence length equals <t_bound>.\n","\n","    Inputs:\n","        <enc_col>: Type <list[list[<int>]]>. An integer-encoded collection of sequences.\n","            Warning: It was assumed that for each list-member of <enc_col>, its final integer-token-element has to be exactly the integer-encoded '<eos>' given by <eos_int>, otherwise the .so file won't be able to process the input.\n","        <eos_int>: Type <int>. The special end-of-sequence token encoded as an integer.\n","        <pad_int>: Type <int>. The special padding token encoded as an integer.\n","        <t_bound>: Type <int>. Number of time steps for the resulting sequences.\n","\n","    Outputs:\n","        <enc_col_>: Type <list[list[<int>]]>. Collection <enc_col>'s sequences where each sequence is padded or trimmed. All sequences have length equal to <t_bound>.\n","    \"\"\"\n","\n","    ## Prototype\n","    lib.pad_or_trim.argtypes = \\\n","    [\n","        ctypes.POINTER(ctypes.POINTER(ctypes.c_int)),\n","        ctypes.c_int,\n","        ctypes.c_int,\n","        ctypes.c_int,\n","        ctypes.c_int,\n","        ctypes.POINTER(ctypes.POINTER(ctypes.c_int))\n","    ]\n","\n","    ## ! Define the arrays' instances and allocate memory for them: Begin\n","\n","    enc_col_ = [[None for word_idx in range(t_bound)] for seq_idx in range(len(enc_col))]\n","\n","    enc_col_array = (ctypes.POINTER(ctypes.c_int) * len(enc_col))()\n","    for i, enc_seq in enumerate(enc_col):\n","        enc_col_array[i] = (ctypes.c_int * len(enc_seq))()\n","\n","    enc_col_array_ = (ctypes.POINTER(ctypes.c_int) * len(enc_col_))()\n","    for i in range(len(enc_col_)):\n","        enc_col_array_[i] = (ctypes.c_int * t_bound)()\n","\n","    ## Allocating memory\n","    for i in range(len(enc_col)):\n","        for j in range(len(enc_col[i])):\n","            enc_col_array[i][j] = enc_col[i][j]\n","\n","    ## ! Define the arrays' instances and allocate memory for them: End\n","\n","    ## Update <enc_col_array_>\n","    lib.pad_or_trim(enc_col_array, len(enc_col), eos_int, pad_int, t_bound, enc_col_array_)\n","\n","    ## Copy values from <enc_col_seq_array> to <enc_col_seq>\n","    for i in range(len(enc_col_)):\n","        for j in range(len(enc_col_[i])):\n","            enc_col_[i][j] = enc_col_array_[i][j]\n","\n","    return enc_col_\n","\n","def build_vocab(seq_string, special_tokens):\n","    \"\"\"\n","    Description:\n","        Generates a vocabulary object with respect to the decreasing order of token-frequency.\n","\n","    Inputs:\n","        <seq_string>: Type: <list[<str>]>. Contains the dataset's tokens.\n","        <special_tokens>: Type: <list[<str>]>. Contains the special tokens.\n","\n","    Outputs:\n","        <vocab_>: Type: <torchtext.vocab.Vocab>.\n","        <freqs>: Type: <list>.\n","    \"\"\"\n","\n","    seq_string_counter = Counter(seq_string)\n","    seq_string_sorted_by_freq_tuples = sorted(seq_string_counter.items(), key=lambda x: x[1], reverse=True)\n","    seq_string_ordered_dict = OrderedDict(seq_string_sorted_by_freq_tuples)\n","    vocab_ = torchtext.vocab.vocab\\\n","    (\n","        ordered_dict = seq_string_ordered_dict,\n","        min_freq = 2,\n","        specials = special_tokens,\n","        special_first = True\n","    )\n","\n","    freqs = list(iter(seq_string_ordered_dict.values()))\n","\n","    return freqs, vocab_\n","\n","def bosify_seq(col_seq_int, bos_int):\n","    '''\n","    Description:\n","        Extremely important function that takes a collection of sequences, and on the step axis, it removes the final (special) token and adds the <bos> token in the beginning of the sequence.\n","\n","    Inputs:\n","        <col_seq_int>: Type: torch.Tensor. Shape: (n_examples, max_steps_seq).\n","\n","    Outputs:\n","        <col_seq_int_bos>: Type: torch.Tensor. Shape: (n_examples, max_steps_seq)\n","    '''\n","\n","    tgt_int_bos = col_seq_int[:,:-1]\n","    bos_int_expanded = torch.ones((col_seq_int.shape[0], 1), dtype=int) * bos_int\n","    col_seq_int_bos = torch.concat((bos_int_expanded, tgt_int_bos), axis=1)\n","\n","    return col_seq_int_bos\n","\n","class translated_text_dataset:\n","\n","    def __init__(self, shuffle_seed):\n","        '''\n","        Input:\n","            <shuffle_seed>: Type <int>. Used as a seed value for the RNG of the associated dataset's splitting. <split_seed> must never change between training interruptions or parameter transfers. The initial seed value should be randomized.\n","        '''\n","\n","        def parse_local_raw_data(raw_dataset_fpath):\n","\n","            with open(raw_dataset_fpath, 'r') as file:\n","                raw_dataset_str = file.read()[:-1]\n","\n","            return raw_dataset_str\n","\n","        self.shuffle_seed = shuffle_seed\n","\n","        self.instance_shuffle_generator = torch.Generator()\n","        self.instance_shuffle_generator.manual_seed(self.shuffle_seed)\n","\n","        raw_dataset_fpath = './drive/MyDrive/colab_storage/datasets/ell.txt'\n","        dataset_dir_fpath = '/'.join(raw_dataset_fpath.split('/')[:-1])\n","        self.dataset_name = raw_dataset_fpath.split('.')[-2].split('/')[-1]\n","\n","        self.max_steps_src = self.max_steps_tgt = 9\n","        self.n_of_instances_to_keep = -1 # For debugging use: 640\n","\n","        ## [training fraction, validation fraction, test fraction].\n","        self.split_fractions = [0.9, 0.1, 0.0]\n","\n","        self.raw_dataset_str = parse_local_raw_data(raw_dataset_fpath)\n","\n","        preprocessed_dataset_fname = 'pp_' + self.dataset_name\n","        self.preprocessed_dataset_fpath = dataset_dir_fpath + '/' + preprocessed_dataset_fname + '.pt'\n","\n","    def generate_dataset(self):\n","        \"\"\"\n","        Description:\n","            Loads raw dataset, conducts basic token preprocessing and generates the feature and target tensors.\n","        \"\"\"\n","\n","        ## Data cleaning\n","        self.dataset_str = purger(self.raw_dataset_str)\n","        self.dataset_str = initial_text_preprocess(self.dataset_str)\n","\n","        ## Tokenization (string form)\n","        self.src_str, self.tgt_str = tokenize(self.dataset_str)\n","\n","        ## Shuffle\n","        perm = torch.randperm(len(self.src_str), generator=self.instance_shuffle_generator).tolist()\n","        self.src_str = [self.src_str[example_idx] for example_idx in perm]\n","        self.tgt_str = [self.tgt_str[example_idx] for example_idx in perm]\n","\n","        self.src_str = self.src_str[:self.n_of_instances_to_keep]\n","        self.tgt_str = self.tgt_str[:self.n_of_instances_to_keep]\n","\n","        ## Vocabulary and token-wise frequency of appearance\n","        src_freqs, self.src_vocab = build_vocab\\\n","        (\n","            [token for sentence in self.src_str for token in sentence],\n","            special_tokens=['<unk>', '<pad>', '<eos>']\n","        )\n","        tgt_freqs, self.tgt_vocab = build_vocab\\\n","        (\n","            [token for sentence in self.tgt_str for token in sentence],\n","            special_tokens=['<unk>', '<pad>', '<eos>', '<bos>']\n","        )\n","\n","        ## Encode tokens from strings to integers\n","        self.src_int = c_numericalize_str(self.src_str, self.src_vocab.get_itos()) ## = X\n","        self.tgt_int = c_numericalize_str(self.tgt_str, self.tgt_vocab.get_itos()) ## = Y\n","\n","        ## Produce plots\n","        plot_sentence_size(data_pair=(self.src_str, self.tgt_str), image_name=self.dataset_name+'_cnt_examples_per_sentence_len')\n","        plot_frequency_curves(freqs_pair=(src_freqs, tgt_freqs), image_name=self.dataset_name+'_freq')\n","\n","        ## Pad or trim and conversion to tensor\n","        self.src_int = torch.tensor\\\n","        (\n","            c_pad_or_trim\\\n","            (\n","                enc_col=self.src_int,\n","                eos_int=self.src_vocab.get_stoi()['<eos>'],\n","                pad_int=self.src_vocab.get_stoi()['<pad>'],\n","                t_bound=self.max_steps_src\n","            ),\n","            # dtype=torch.int32\n","        )\n","        self.tgt_int = torch.tensor\\\n","        (\n","            c_pad_or_trim\\\n","            (\n","                enc_col=self.tgt_int,\n","                eos_int=self.tgt_vocab.get_stoi()['<eos>'],\n","                pad_int=self.tgt_vocab.get_stoi()['<pad>'],\n","                t_bound=self.max_steps_tgt\n","            ),\n","            # dtype=torch.int32 ## This leads to an error in <torch.nn.functional.one_hot> because <self.tgt_int> stops being an \"index tensor\" if its data type is a 32-bit integer.\n","        )\n","\n","        # self.eos_test()\n","        # self.test_correct_sentence()\n","\n","        self.n_instances = self.src_int.shape[0]\n","\n","        self.src_vocab_size = len( self.src_vocab.get_itos() )\n","        self.tgt_vocab_size = len( self.tgt_vocab.get_itos() )\n","\n","        self.tgt_int_bos = bosify_seq(col_seq_int=self.tgt_int, bos_int=self.tgt_vocab.get_stoi()['<bos>'])\n","\n","        self.dataset = torch.utils.data.TensorDataset(self.src_int, self.tgt_int, self.tgt_int_bos)\n","\n","        separateXy = lambda set: ( torch.stack([set[i][_] for i in range(len(set))], axis=0) for _ in range(2) )\n","\n","        if self.split_fractions[-1] != 0: # Included a test set\n","            self.train_set, self.val_set, self.test_set = torch.utils.data.random_split(self.dataset, self.split_fractions, generator=torch.Generator().manual_seed(42))\n","\n","            self.n_train = len(self.train_set)\n","            self.n_val = len(self.val_set)\n","            self.n_test = len(self.test_set)\n","\n","            self.X_train, self.y_train = separateXy(self.train_set)\n","            self.X_val, self.y_val = separateXy(self.val_set)\n","            self.X_test, self.y_test = separateXy(self.test_set)\n","\n","            dataset = {'train': self.train_set, 'val': self.val_set, 'test': self.test_set}\n","\n","        else: # Excluded the test set\n","            self.train_set, self.val_set = torch.utils.data.random_split(self.dataset, self.split_fractions[:2], generator=torch.Generator().manual_seed(42))\n","\n","            self.n_train = len(self.train_set)\n","            self.n_val = len(self.val_set)\n","\n","            self.X_train, self.y_train = separateXy(self.train_set)\n","            self.X_val, self.y_val = separateXy(self.val_set)\n","\n","            dataset = {'train': self.train_set, 'val': self.val_set, 'test': None}\n","\n","        torch.save(obj=dataset, f=self.preprocessed_dataset_fpath)\n","\n","    def eos_test(self):\n","        '''\n","        Description:\n","            <eos> token exists exactly once iff it's a pass.\n","        '''\n","\n","        pass_eos_test = True\n","        for i in range(self.src_int.shape[0]):\n","            exactly_one = (Counter(self.src_int[i,:].tolist())[self.src_vocab.get_stoi()['<eos>']] == 1) and (Counter(self.tgt_int[i,:].tolist())[self.tgt_vocab.get_stoi()['<eos>']] == 1)\n","            pass_eos_test = pass_eos_test and exactly_one\n","\n","        if pass_eos_test:\n","            print('eos test passed')\n","        else:\n","            print('eos test failed')\n","\n","    def test_correct_sentence(self):\n","\n","        # translated_int = torch.argmax(p_distr, axis=2)[0]\n","        translated_str_src = [self.src_vocab.get_itos()[self.src_int[100,t]] for t in range(self.src_int.shape[1])]\n","        translated_str_tgt = [self.tgt_vocab.get_itos()[self.tgt_int[100,t]] for t in range(self.tgt_int.shape[1])]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tsczPN7aw2NJ"},"outputs":[],"source":["def xavier_uniform(n_in, n_out, device):\n","\n","    bound = math.sqrt(3.0) * math.sqrt(2.0 / float(n_in + n_out))\n","    return nn.Parameter(torch.rand((n_in, n_out)).to(device)*(2*bound)-bound)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECrN_k3dy8px"},"outputs":[],"source":["class dense(nn.Module):\n","\n","    def __init__(self, n_inp, n_out, n_steps, device, dropout_rate=0.0):\n","\n","        super().__init__()\n","\n","        self.device = device\n","\n","        self.n_inp = n_inp\n","        self.n_out = n_out\n","        self.n_steps = n_steps\n","\n","        self.dropout_rate = dropout_rate\n","\n","        self.W = xavier_uniform(self.n_inp, self.n_out, self.device)\n","        self.b = nn.Parameter(torch.zeros(self.n_out).to(self.device))\n","\n","        self.name = 'dense'\n","\n","    def forward(self, inputs, train):\n","        '''\n","        Inputs:\n","            <inputs>: Type: <torch.Tensor>. Shape: (n_examples, n_steps, n_inp) or (n_examples, n_inp).\n","            <train>: Type: <bool>.\n","\n","        Outputs:\n","            <outputs>: Type: <torch.Tensor>. Shape: (n_examples, n_steps, n_out) if len(inputs.shape) is 3 or (n_examples, n_out) if len(inputs.shape) is 3.\n","        '''\n","\n","        input_shape_length = len(inputs.shape)\n","        if input_shape_length == 2:\n","            ## Expanding inputs to a tensor with shape (n_examples, 1, n_inp)\n","            inputs = inputs[:, None, :]\n","\n","        outputs = []\n","        for (t, X_t) in enumerate(inputs.swapaxes(0,1)):\n","            O_t = torch.matmul(X_t, self.W) + self.b\n","            outputs.append(O_t)\n","        outputs = torch.stack(outputs).swapaxes(0,1)\n","\n","        if train and (self.dropout_rate != 0.0):\n","            switches = torch.ones(outputs.shape[2]).to(self.device)\n","            switches[0:round(self.dropout_rate * outputs.shape[2])] = 0.0\n","            switches = switches[torch.randperm(outputs.shape[2])]\n","\n","            switches_outputs = switches.repeat(outputs.shape[0], outputs.shape[1], 1)\n","            outputs = outputs * switches_outputs\n","\n","        if input_shape_length == 2:\n","            outputs = outputs[:, 0, :]\n","\n","        return outputs\n","\n","    def __repr__(self):\n","\n","        n_pars = 0\n","        for par in self.parameters():\n","            n_pars += math.prod(par.shape)\n","\n","        return f\"%s(n_params=%d, n_inp=%d, n_out=%d, n_steps=%d)\"%(self.name, n_pars, self.n_inp, self.n_out, self.n_steps)\n","\n","class vanilla_recurrent(nn.Module):\n","    \"\"\"\n","    Description:\n","        Vanilla recurrent layer.\n","    \"\"\"\n","\n","    def __init__(self, n_inp, n_hid, device):\n","\n","        super().__init__()\n","\n","        self.device = device\n","\n","        self.n_inp = n_inp\n","        self.n_hid = n_hid\n","\n","        self.W_xh = xavier_uniform(self.n_inp, self.n_hid, self.device)\n","        self.W_hh = xavier_uniform(self.n_hid, self.n_hid, self.device)\n","        self.b_h = nn.Parameter(torch.zeros(n_hid).to(self.device))\n","\n","        self.name = 'vanilla_recurrent'\n","\n","    def forward(self, inputs, H_t=None):\n","        \"\"\"\n","        Inputs:\n","            <inputs>: Type: <torch.Tensor>. Shape: (n_minibatch, n_steps, n_inp).\n","            <H_t>: Type: <torch.Tensor>. Shape: (n_minibatch, n_hid).\n","\n","        Outputs:\n","            <outputs>: Type: torch.Tensor. Shape: (n_minibatch, n_steps, n_hid).\n","        \"\"\"\n","\n","        if H_t is None:\n","            H_t = torch.zeros((inputs.shape[0], self.n_hid)).to(self.device)\n","        outputs = []\n","        for (t, X_t) in enumerate(inputs.swapaxes(0, 1)):\n","            H_t = torch.tanh(torch.matmul(X_t, self.W_xh) + torch.matmul(H_t, self.W_hh) + self.b_h)\n","            outputs.append(H_t)\n","        outputs = torch.stack(outputs).swapaxes(0, 1)\n","\n","        return outputs, H_t\n","    \n","    def __repr__(self):\n","\n","        n_pars = 0\n","        for par in self.parameters():\n","            n_pars += math.prod(par.shape)\n","\n","        return f\"%s(n_params=%d)\"%(self.name, n_pars)\n","\n","class gru(nn.Module):\n","\n","    def __init__(self, n_inp, n_hid, device, dropout_rate=0.0):\n","\n","        def init_weight(n_in, n_out):\n","\n","            bound = math.sqrt(3.0) * math.sqrt(2.0 / float(n_in + n_out))\n","\n","            return nn.Parameter(torch.rand((n_in, n_out)).to(self.device)*(2*bound)-bound)\n","\n","        super().__init__()\n","\n","        self.device = device\n","\n","        self.n_inp = n_inp\n","        self.n_hid = n_hid\n","\n","        self.dropout_rate = dropout_rate\n","\n","        triple = lambda: \\\n","        (\n","            xavier_uniform(self.n_inp, self.n_hid, self.device),\n","            xavier_uniform(self.n_hid, self.n_hid, self.device),\n","            nn.Parameter(torch.zeros(self.n_hid).to(self.device))\n","        )\n","        self.W_xz, self.W_hz, self.b_z = triple() # Update gate\n","        self.W_xr, self.W_hr, self.b_r = triple() # Reset gate\n","        self.W_xh, self.W_hh, self.b_h = triple() # Candidate hidden state\n","\n","        self.name = 'gru'\n","\n","    def forward(self, inputs, train, H_t=None):\n","        '''\n","        Inputs:\n","            <inputs>: Type: <torch.Tensor>. Shape: (n_examples, n_steps, vocab_size).\n","            <train>: Type: <bool>. Toggles training/predicting.\n","            <H_t>: Type: <torch.Tensor>. Shape: (n_examples, n_hid). The initial hidden state. Default is None.\n","\n","        Outputs:\n","            <outputs_>: Type: <tuple(<outputs>, <H_t>)>. Assuming that T=n_steps.\n","                <outputs>: <torch.Tensor>. Shape: (n_examples, n_steps, n_hid). Hidden states for all t=0,1,...,T.\n","                <H_t>: <torch.Tensor>. Shape: (n_examples, n_hid). Final hidden state at t=T.\n","        '''\n","\n","        if H_t is None:\n","            # Initial state with shape: (self.batch_size, self.n_hid)\n","            H_t = torch.zeros((inputs.shape[0], self.n_hid), device=self.device)\n","        outputs = []\n","        for (t, X_t) in enumerate(inputs.swapaxes(0, 1)):\n","            Z_t = torch.sigmoid(torch.matmul(X_t, self.W_xz) + torch.matmul(H_t, self.W_hz) + self.b_z)\n","            R_t = torch.sigmoid(torch.matmul(X_t, self.W_xr) + torch.matmul(H_t, self.W_hr) + self.b_r)\n","            H_tilde_t = torch.tanh(torch.matmul(X_t, self.W_xh) + torch.matmul(R_t * H_t, self.W_hh) + self.b_h)\n","            H_t = Z_t * H_t + (1 - Z_t) * H_tilde_t\n","            outputs.append(H_t)\n","        outputs = torch.stack(outputs).swapaxes(0, 1)\n","\n","        if train and (self.dropout_rate != 0.0):\n","\n","            switches = torch.ones(self.n_hid).to(self.device)\n","            switches[0:round(self.dropout_rate * self.n_hid)] = 0.0\n","            switches = switches[torch.randperm(self.n_hid)]\n","\n","            switches_outputs = switches.repeat(outputs.shape[0], outputs.shape[1], 1)\n","            outputs = outputs * switches_outputs\n","\n","            switches_H_t = switches.repeat(H_t.shape[0], 1)\n","            H_t = H_t * switches_H_t\n","\n","        return outputs, H_t\n","\n","    def __repr__(self):\n","\n","        n_pars = 0\n","        for par in self.parameters():\n","            n_pars += math.prod(par.shape)\n","\n","        return f\"%s(n_params=%d, n_inp=%d, n_hid=%d)\"%(self.name, n_pars, self.n_inp, self.n_hid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLR7ynbyw2NL"},"outputs":[],"source":["class encoder(nn.Module):\n","\n","    def __init__(self, n_inp, n_max_steps, n_out, device):\n","        '''\n","        Description:\n","            The encoder of a MT module. The final layer is a recurrent type.\n","\n","        Inputs:\n","            <n_inp>: Type <int>. The number of input neurons per example. Coincides with the size of the source vocabulary.\n","            <n_max_steps>: Type <int>. The number of source's steps including the padding size.\n","            <n_out>: Type <int>. For a fixed example and step, this is the number of neurons in the encoder's output layer.\n","            <device>: Type <torch.device>.\n","        '''\n","\n","        super().__init__()\n","\n","        self.device = device\n","\n","        ## Number of embedding axes.\n","        self.n_emb = 600\n","\n","        self.n_hid1 = 600\n","\n","        self.n_inp = n_inp\n","        self.n_max_steps = n_max_steps\n","\n","        ## Output neurons scaler. May as well be named as \"self.n_hid\".\n","        self.n_out = n_out\n","\n","        self.emb = dense(n_inp=self.n_inp, n_out=self.n_emb, n_steps=self.n_max_steps, device=self.device)\n","        self.rec1 = gru(n_inp=self.n_emb, n_hid=self.n_hid1, device=self.device, dropout_rate=0.0)\n","        self.rec2 = gru(n_inp=self.n_hid1, n_hid=self.n_out, device=self.device, dropout_rate=0.0)\n","\n","    def forward(self, X, train):\n","        '''\n","        Inputs:\n","            <X>: Type: <torch.Tensor>. Shape: (n_examples, n_steps, vocab_size). The encoder's input which is the preprocessed source data.\n","            <train>: Type: <bool>.\n","\n","        Outputs:\n","            <out>: Type: <tuple[<torch.Tensor>, <torch.Tensor>]>.\n","                <output_of_final_layer>: Type: <torch.Tensor>. Shape: (n_examples, n_steps, n_out). The output of the final recurrent layer containing all steps.\n","                <layerwise_final_states>: Type: <torch.Tensor>. Shape: (n_recurrent_layers, n_examples, n_out). Along the first axis, it contains the final state of each recurrent layer in this module.\n","        '''\n","\n","        recurrent_layers_final_states = []\n","\n","        out = self.emb(inputs=X, train=train)\n","        out = self.rec1(inputs=out, train=train)\n","        recurrent_layers_final_states.append(out[1])\n","        out = self.rec2(inputs=out[0], train=train)\n","        recurrent_layers_final_states.append(out[1])\n","\n","        return (out[0], recurrent_layers_final_states)\n","\n","class decoder(nn.Module):\n","\n","    def __init__(self, n_tgt_inp, n_context, n_max_steps, n_out, bos_int, eos_int, pad_int, device):\n","        '''\n","        Inputs:\n","            <n_tgt_inp>: Type <int>. The target's size per example per step.\n","            <n_context>: Type <int>. The context's size.\n","            <n_max_steps>: Type <int>. The number of target's steps including the padding size.\n","            <n_out>: Type <int>. Size of the output dense layer. Equal to the size of the source's vocabulary.\n","            <device>: Type <torch.device>.\n","        '''\n","\n","        super().__init__()\n","\n","        self.device = device\n","\n","        self.n_emb = 600\n","        self.n_hid1 = 600\n","        self.n_hid2 = 600\n","\n","        self.bos_int = bos_int\n","        self.eos_int = eos_int\n","        self.pad_int = pad_int\n","        self.n_tgt_inp = n_tgt_inp\n","        self.n_context = n_context\n","        self.n_inp = self.n_tgt_inp\n","        self.n_yc = self.n_context + self.n_emb\n","        self.n_out = n_out\n","        self.n_max_steps = n_max_steps\n","\n","        self.tgt_ohe_modifier = lambda x: torch.stack([F.one_hot(x[seq_idx], self.n_tgt_inp) for seq_idx in range(len(x))], axis=0).type(torch.float32).to(self.device)\n","\n","        self.emb = dense(n_inp=self.n_inp, n_out=self.n_emb, n_steps=self.n_max_steps, device=self.device)\n","        self.rec1 = gru(n_inp=self.n_yc, n_hid=self.n_hid1, device=self.device, dropout_rate=0.0)\n","        self.rec2 = gru(n_inp=self.n_hid1, n_hid=self.n_hid2, device=self.device, dropout_rate=0.0)\n","        self.dense_out = dense(n_inp=self.n_hid2, n_out=self.n_out, n_steps=self.n_max_steps, device=self.device)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","        self.available_pipelines = \\\n","        [\n","            self.train_teacher_forcing_pipeline,\n","            self.train_scheduled_sampling_pipeline,\n","            self.prediction_stochastic_pipeline,\n","            self.prediction_greedy_search_pipeline,\n","            self.prediction_beam_search_pipeline\n","        ]\n","\n","        self.eos_ohe = self.tgt_ohe_modifier(torch.tensor(eos_int)[None, None])\n","        self.pad_ohe = self.tgt_ohe_modifier(torch.tensor(pad_int)[None, None])\n","\n","    ## ! Architecture: Begin\n","\n","    def one_step_primary_pipeline(self, inp, context, initial_states, train):\n","        rec1_init_state, rec2_init_state = initial_states\n","\n","        out_ = inp\n","        out_ = self.emb(inputs=out_, train=train)\n","        out_ = torch.cat((out_, context), -1)\n","        out_ = self.rec1(inputs=out_, H_t=rec1_init_state, train=train)\n","        rec1_init_state = out_[1]\n","        out_ = self.rec2(inputs=out_[0], H_t=rec2_init_state, train=train)\n","        rec2_init_state = out_[1]\n","        out_ = self.dense_out(inputs=out_[0], train=train)\n","        out_ = self.softmax(out_)\n","\n","        return out_, (rec1_init_state, rec2_init_state)\n","\n","    def multi_step_primary_pipeline(self, Y, context, initial_states, train):\n","        rec1_init_state, rec2_init_state = initial_states\n","\n","        out = self.emb(inputs=Y, train=train)\n","        out = torch.cat((out, context), -1)\n","        out = self.rec1(inputs=out, H_t=rec1_init_state, train=train)\n","        out = self.rec2(inputs=out[0], H_t=rec2_init_state, train=train)\n","        out = self.dense_out(inputs=out[0], train=train)\n","        out = self.softmax(out)\n","\n","        return out\n","\n","    ## ! Architecture: End\n","\n","    ## ! Wraps of the primary/main decoder's pipeline: Begin\n","\n","    def train_teacher_forcing_pipeline(self, Y, context, initial_states, train, dec_config):\n","        '''\n","        Trigger:\n","            When <dec_mode> is 0.\n","\n","        Description:\n","            Returns a distribution function. Given a t \\in \\{ 1, ..., T-1 \\}, it's\n","            P(Y_t | Y_{t-1}=y_{t-1}).\n","        '''\n","\n","        out = self.multi_step_primary_pipeline(Y=Y, context=context, initial_states=initial_states, train=train)\n","\n","        return out\n","\n","    def train_scheduled_sampling_pipeline(self, Y, context, initial_states, train, dec_config):\n","        '''\n","        Trigger:\n","            When <dec_mode> is 1.\n","\n","        Description:\n","            Returns a distribution function. Given a t \\in \\{ 1, ..., T-1 \\}, it's\n","            P(Y_t | Y_{t-1}=y_{t-1}).\n","        '''\n","\n","        decay = dec_config['decay']\n","\n","        switches = torch.ones(self.n_max_steps).type(torch.bool)\n","        switches[0:round((1-decay) * (self.n_max_steps))] = False\n","        switches = [True] + switches[torch.randperm(self.n_max_steps)].tolist()\n","\n","        context = context[:,0:1,:]\n","\n","        out = []\n","        for t in range(self.n_max_steps):\n","            if switches[t]:\n","                out_ = Y[:,t:t+1,:]  ## Remember that the first value is <bos> !!!\n","\n","            out_, initial_states = self.one_step_primary_pipeline(inp=out_, context=context, initial_states=initial_states, train=False)\n","            out.append(out_[:,0,:]) ## To justify the slice out_[:,0,:]: <out_> has (n_examples, 1, n_tgt_inp), and we're iterating with respect to the step t. When the loops terminates, <out> will properly contain the time step axis.\n","        out = torch.stack(out).swapaxes(0, 1)\n","\n","        return out\n","\n","    def prediction_stochastic_pipeline(self, Y, context, initial_states, train, dec_config):\n","        '''\n","        Trigger:\n","            When <dec_mode> is 2.\n","\n","        Description:\n","            Returns a distribution function.\n","            P(Y_t | Y_{t-1}, ..., Y_0, CONTEXT=context)\n","        '''\n","\n","        out = []\n","        out_ = Y\n","        for t in range(self.n_max_steps):\n","            out_, initial_states = self.one_step_primary_pipeline(inp=out_, context=context, initial_states=initial_states, train=False)\n","            out.append(out_[:,0,:]) ## To justify the slice out_[:,0,:]: <out_> has (n_examples, 1, n_tgt_inp), and we're iterating with respect to the step t. When the loops terminates, <out> will properly contain the time step axis.\n","        out = torch.stack(out).swapaxes(0, 1)\n","\n","        return out\n","\n","    def prediction_greedy_search_pipeline(self, Y, context, initial_states, train, dec_config):\n","        '''\n","        Trigger:\n","            When <dec_mode> is 3.\n","\n","        Description:\n","            Returns a fixed token\n","            x_{t+1}\n","            inferred from\n","            P(X_{t+1} | X_t=x_t, ..., X_0=x_0, CONTEXT=context)\n","        '''\n","\n","        out = []\n","        out_ = Y\n","        for t in range(self.n_max_steps):\n","            out_, initial_states = self.one_step_primary_pipeline(inp=out_, context=context, initial_states=initial_states, train=False)\n","            out.append(out_[:,0,:])\n","            out_ = torch.argmax(out_, axis=-1)\n","            out_ = self.tgt_ohe_modifier(out_)\n","        out = torch.stack(out).swapaxes(0, 1)\n","\n","        return out\n","\n","    def prediction_beam_search_pipeline(self, Y, context, initial_states, train, dec_config):\n","        '''\n","        Trigger:\n","            When <dec_mode> is 4.\n","\n","        Description:\n","            Returns a fixed token.\n","        '''\n","\n","        def score(probability, seq_length):\n","\n","            a = 0.75\n","            score_value = np.log(probability) / seq_length**a\n","\n","            return score_value\n","\n","        def get_scores(probabilities):\n","\n","            scores = [[None for branch_idx in range(len(probabilities[t]))] for t in range(len(probabilities))]\n","\n","            for t in range(len(probabilities)):\n","                seq_length = t + 1\n","                for branch_idx in range(len(probabilities[t])):\n","                    scores[t][branch_idx] = score(probabilities[t][branch_idx], seq_length)\n","\n","            return scores\n","\n","        def get_optimal_score_position(score):\n","\n","            n_examples = len(score[0][0])\n","\n","            max_score = [-np.infty for i in range(n_examples)]\n","            t_optimal = [None for i in range(n_examples)]\n","            branch_idcs_optimal = [None for i in range(n_examples)]\n","\n","            for i in range(n_examples):\n","\n","                for t in range(len(score)):\n","                    for branch_idx in range(len(score[t])):\n","                        if score[t][branch_idx][i] > max_score[i]:\n","                            max_score[i] = score[t][branch_idx][i]\n","                            t_optimal[i] = t\n","                            branch_idcs_optimal[i] = branch_idx\n","\n","            return t_optimal, branch_idcs_optimal\n","\n","        def get_optimal_col_sequences(beam_graph, t_optimal, branch_idcs_optimal, eos_ohe, pad_ohe):\n","            '''\n","            Description:\n","                Finalizes the prediction tensor containing the optimal sequences among others based on given indices.\n","\n","            Inputs:\n","                <beam_graph>: Type: <list[<list[torch.Tensor]>]>. The outer list has length equal to n_max_steps. At the outer list's position beam_step, the inner list has length equal to the number of branches there. The tensor inside has shape (n_examples, 1, n_out). Hence a value can be indexed like this\n","                e.g. beam_graph[beam_step][branch_idx][example_idx, 0, out_dim]\n","                <t_optimal>: Type: <list[<int>]>. Length: n_examples. For a given example, this list contains the steps where beam_graph contains the optimal sequence.\n","                <branch_idcs_optimal>: Type: <list[<int>]>. Length: n_examples. For a given example, this list contains the branch index (relative to its corresponding step) where beam_graph contains the optimal sequence.\n","                <eos_ohe>: Type: <torch.Tensor>. Shape: (1, 1, n_out). The end of sequence token (<eos>) in OHE.\n","                <pad_ohe>: Type: <torch.Tensor>. Shape: (1, 1, n_out). The padding token (<pad>) in OHE.\n","\n","            Outputs:\n","                <optimal_col_sequences>: Type: <torch.Tensor>. Shape: (n_examples, max_n_steps, n_out).\n","            '''\n","\n","            n_examples = len(t_optimal)\n","\n","            optimal_col_sequences = []\n","            for i in range(n_examples):\n","\n","                ## Find optimal sequences\n","                optimal_sequence = beam_graph[t_optimal[i]][branch_idcs_optimal[i]][i:i+1] ## The tensor has shape (x1 example, t_optimal[i]+1, n_out).\n","\n","                ## ! Fill in each sequence to achieve shape homogeneity: Begin\n","\n","                empty_positions = self.n_max_steps - optimal_sequence.shape[1]\n","\n","                # optimal_col_sequences.append(optimal_sequence)\n","                if empty_positions > 0:\n","                    optimal_sequence = torch.cat((optimal_sequence, eos_ohe, pad_ohe.repeat(1, empty_positions-1, 1)), axis=1)\n","\n","                ## ! Fill in each sequence to achieve shape homogeneity: Begin\n","\n","                optimal_col_sequences.append(optimal_sequence)\n","\n","            optimal_col_sequences = torch.cat(optimal_col_sequences, axis=0)\n","\n","            return optimal_col_sequences\n","\n","        beam_width = dec_config['beam_width']\n","        assert isinstance(beam_width, int) and (beam_width >= 1) and (beam_width <= self.n_out), \"E: Invalid beam width value.\"\n","\n","        beam_graph = [[None for parent_branch_idx in range(min(beam_width, t*beam_width+1))] for t in range(self.n_max_steps+1)]\n","        beam_graph_probabilities = [[None for parent_branch_idx in range(min(beam_width, t*beam_width+1))] for t in range(self.n_max_steps+1)]\n","        beam_graph[0][0] = Y\n","        beam_graph_probabilities[0][0] = np.ones((Y.shape[0], 1)) ## Shape: (n_examples, 1 n_possibilities@t=0)\n","\n","        for t in range(self.n_max_steps): ## Referring to the branch specified by <t> and <parent_branch_idx> as the parent branch, and all the brances connected with this parent branch that belong to step, <t>+1 as child branches.\n","            for parent_branch_idx in range(len(beam_graph[t])):\n","\n","                ## ! Forward run: Begin\n","\n","                out_ = beam_graph[t][parent_branch_idx][:,-1,:][:, None, :]\n","                out_, initial_states = self.one_step_primary_pipeline(inp=out_, context=context, initial_states=initial_states, train=False)\n","\n","                beam_graph_node = out_\n","\n","                ## ! Forward run: End\n","\n","                ## ! Beam Search graph update: Begin\n","\n","                if t == 0:\n","                    k = deepcopy(beam_width)\n","                else:\n","                    k = 1\n","\n","                child_branches_token_probability_given_parent_token, child_branches_likely_tokens_int = torch.topk(input=beam_graph_node, k=k, dim=-1) ## Shapes are both (n_examples, 1 step, k)\n","                child_branches_token_probability_given_parent_token = child_branches_token_probability_given_parent_token[:,0,:].cpu().detach().numpy()\n","                child_branches_likely_tokens_ohe = self.tgt_ohe_modifier(child_branches_likely_tokens_int) ## Shape (n_examples, 1 step, k, n_out)\n","                parent_branch_built_sequence_probability = np.repeat(beam_graph_probabilities[t][parent_branch_idx], k, axis=1)\n","\n","                child_branches_built_sequence_probability = parent_branch_built_sequence_probability * child_branches_token_probability_given_parent_token ## Probability chain rule\n","\n","                child_step_connected_branch_relative_to_parent_idx = 0\n","                for child_step_connected_branch_idx in range(k*parent_branch_idx, k*(parent_branch_idx+1)):\n","                    beam_graph[t+1][child_step_connected_branch_idx] = torch.cat((beam_graph[t][parent_branch_idx], child_branches_likely_tokens_ohe[..., child_step_connected_branch_relative_to_parent_idx, :]), -2) ## Concatenating all tokens produced until this step, for this possibility. Shape (n_examples, (t+1)+1, n_out)\n","                    beam_graph_probabilities[t+1][child_step_connected_branch_idx] = child_branches_built_sequence_probability[..., child_step_connected_branch_relative_to_parent_idx:child_step_connected_branch_relative_to_parent_idx+1]\n","                    child_step_connected_branch_relative_to_parent_idx += 1\n","                del child_step_connected_branch_relative_to_parent_idx\n","\n","                ## ! Beam Search graph update: End\n","\n","        beam_graph = beam_graph[1:]\n","\n","        ## ! Dropping <bos> entirely: Begin\n","\n","        for t in range(self.n_max_steps):\n","            for parent_branch_idx in range(len(beam_graph[t])):\n","                beam_graph[t][parent_branch_idx] = beam_graph[t][parent_branch_idx][:,1:,:]\n","\n","        beam_graph_probabilities = beam_graph_probabilities[1:]\n","\n","        ## ! Dropping <bos> entirely: End\n","\n","        ## ! Search optimal sequence on graph: Begin\n","\n","        scores = get_scores(beam_graph_probabilities)\n","        t_optimal, branch_idcs_optimal = get_optimal_score_position(scores)\n","        optimal_col_sequences = get_optimal_col_sequences\\\n","        (\n","            beam_graph=beam_graph,\n","            t_optimal=t_optimal,\n","            branch_idcs_optimal=branch_idcs_optimal,\n","            eos_ohe=self.eos_ohe,\n","            pad_ohe=self.pad_ohe\n","        )\n","\n","        ## ! Search optimal sequence on graph: End\n","\n","        return optimal_col_sequences\n","\n","    ## ! Wraps of the primary/main decoder's pipeline: End\n","\n","    def forward(self, Y, context, initial_states, dec_mode, dec_config, train):\n","        '''\n","        Inputs:\n","            <Y>: Type: <torch.Tensor> or <NoneType>. Shape: (n_examples, max_steps_tgt+1, n_tgt_inp). Targets of collection-examples in OHE starting with the <bos> token.\n","            <context>: Type: <torch.Tensor>. Shape: (n_examples, Y.shape[1], n_context). Context tensor. Repeated across axis 1.\n","            <initial_states>: Type: <torch.Tensor>. Shape: (n_recurrent_layers, n_examples, n_out). Assumed that the number of recurrent layers in the encoder is the same as in the decoder. Additionally it was assumed that the configuration of these layers are identical.\n","            <dec_mode>: Type: <int>. Specifies which of the decoder's pipeline will be utilized\n","                0: Teacher forcing pipeline\n","                2: Stochastic model sampling\n","                3: Greedy Search\n","                4: Beam Search\n","            <dec_config>: Type: <dict>. Contains the configuration values of its respective pipeline.\n","            <train>: Type: <bool>.\n","\n","        Outputs:\n","            <out>: Output of corresponding pipeline.\n","        '''\n","\n","        pipeline = self.available_pipelines[dec_mode]\n","\n","        out = pipeline(Y=Y, context=context, initial_states=initial_states, dec_config=dec_config, train=train)\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4DWHqcpy8p2"},"outputs":[],"source":["class seq2seq(nn.Module):\n","    '''\n","    Description:\n","        This is the Sequence to Sequence model based on some encoder and decoder.\n","\n","    Specification (High abstraction):\n","        (S1) This pipeline's training process utilizes the Teacher-Forcing methodology.\n","        (S2) The context tensor is set to equal the final state of the encoder's output layer (which is a recurrent layer).\n","        (S3) In case that the model is being trained, for each given decoder's state, the input is a concatenation of the target tensor slice and the context tensor. Otherwise if the model is used to predict, for each given decoder's state, the input is a concatenation of a previous state's output with the context tensor. The decoder's initial state is the OHE of '<bos>'.\n","    '''\n","\n","    def __init__(self, n_src_inp, max_steps_src, n_tgt_inp, max_steps_tgt, bos_int, eos_int, pad_int, device):\n","        '''\n","        Inputs:\n","            <n_src_inp>: Type <int>. The number of source inputs per example per step.\n","            <max_steps_src>: Type <int>. The number source's steps including the padding size.\n","            <n_tgt_inp>: Type <int>. The number of target inputs per example per step.\n","            <max_steps_tgt>: Type <int>. The number target's steps including the padding size.\n","            <bos_int>: Type <int>. The '<bos>' token expressed as an integer i.e. encoded as the target vocabularies index.\n","            <eos_int>: Type <int>. The '<eos>' token expressed as an integer i.e. encoded as the target vocabularies index.\n","            <pad_int>: Type <int>. The '<pad>' token expressed as an integer i.e. encoded as the target vocabularies index.\n","            <device>: Type <torch.device>.\n","        '''\n","\n","        super().__init__()\n","\n","        self.device = device\n","\n","        ## Input size, coiciding with the respective vocabularies' lengths.\n","        self.n_src_inp = n_src_inp\n","        self.n_tgt_inp = n_tgt_inp\n","\n","        ## Number of steps including all paddings.\n","        self.max_steps_src = max_steps_src\n","        self.max_steps_tgt = max_steps_tgt\n","\n","        self.bos_int = bos_int\n","        self.eos_int = eos_int\n","        self.pad_int = pad_int\n","\n","        ## These assignments are established by (S2). The \"enc\" in the name \"enc_n_out\" refers to the encoder.\n","        self.context_n = self.enc_n_out = 600\n","\n","        self.encoder = encoder(n_inp=self.n_src_inp, n_max_steps=self.max_steps_src, n_out=self.enc_n_out, device=self.device)\n","        self.decoder = decoder(n_tgt_inp=self.n_tgt_inp, n_context=self.context_n, n_max_steps=self.max_steps_tgt, n_out=self.n_tgt_inp, device=self.device, bos_int=self.bos_int, eos_int=self.eos_int, pad_int=self.pad_int)\n","\n","        self.name = 'seq2seq'\n","\n","    def init_state(self, enc_output):\n","        '''\n","        Description:\n","            Constructs the context tensor. Assumed that the encoder returns the output of a recurrent layer along with their final states.\n","\n","        Inputs:\n","            <enc_output>: Type: <tuple[<torch.Tensor>, <torch.Tensor>]>. The context tensor.\n","                <output_of_enc_final_layer>: Type: <torch.Tensor>. Shape: (n_examples, n_steps, n_out). The output of the final recurrent layer in the encoder containing all steps.\n","                <layerwise_final_states>: Type: <torch.Tensor>. Shape: (n_recurrent_layers, n_examples, n_out). Along the first axis, it contains the final state of each of the encoder's recurrent layers.\n","\n","        Outputs:\n","            <context>: Type: <tuple[<torch.Tensor>, <torch.Tensor>]>. The context tensor.\n","                <context>: Type: <torch.Tensor>. Shape: (n_examples, n_steps, n_out). The output of the final recurrent layer in the encoder containing all steps.\n","                <layerwise_final_states>: Type: <torch.Tensor>. Shape: (n_recurrent_layers, n_examples, n_out). Along the first axis, it contains the final state of each of the encoder's recurrent layers.\n","        '''\n","\n","        return enc_output\n","\n","    def forward(self, dec_mode, dec_config, train, X, Y=None):\n","        '''\n","        Inputs:\n","            <dec_mode>: Type: <int>. Specifies which of the decoder's pipeline will be utilized.\n","            <dec_config>: Type: <dict>. Decoder's pipeline configuration.\n","            <train>: Type: <bool>.\n","            <X>: Type: <torch.Tensor>. Shape: (n_examples, max_steps_src, n_src_inp). Sources of collection-examples in OHE.\n","            <Y>: Type: <torch.Tensor> or <NoneType>. Shape: (n_examples, max_steps_tgt+1, n_tgt_inp). Targets of collection-examples in OHE starting with the <bos> token.\n","\n","        Outputs:\n","            <Y_hat>: Type: <torch.Tensor>. Shape (n_examples, max_steps_tgt, n_tgt_inp). Predicted sequence. Estimation of Y.\n","        '''\n","\n","        enc_out = self.encoder(X=X, train=train)\n","        _, initial_states = self.init_state(enc_out)\n","        context = initial_states[-1]\n","\n","        if Y == None:\n","            bos_ohe = torch.zeros((X.shape[0], 1, self.n_tgt_inp), device=self.device)\n","            bos_ohe[:,:,self.bos_int] = 1\n","            Y = bos_ohe\n","\n","        context = context.repeat(Y.shape[1], 1, 1).swapaxes(0, 1)\n","\n","        Y_hat = self.decoder(Y=Y, context=context, initial_states=initial_states, dec_mode=dec_mode, dec_config=dec_config, train=train)\n","\n","        return Y_hat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D9CzoLBVy8p4"},"outputs":[],"source":["def categ_cross_entropy(p_distr_pred, p_distr_ground_truth, device, ignore_index=None):\n","    '''\n","    Inputs:\n","        <p_distr_pred>: Type: <torch.Tensor>. Shape: (n_examples, max_steps_tgt, tgt_vocab_size). Estimation of probability distribution.\n","        <p_distr_ground_truth>: Type: <torch.Tensor>. Shape: (n_examples, max_steps_tgt, tgt_vocab_size). Underlying probability distribution.\n","        <ignore_index>: Type: <int> or <NoneType>. If <ignore_index> is an integer then the loss function's computation will exclude the class with index <ignore_index>. In case <ignore_index> is set to None then the loss function will be computed normally, i.e. over all indices of <p_distr_pred> and <p_distr_ground_truth>. By default <ignore_index> is set to None.\n","\n","    Outputs:\n","        <loss_>: Type <torch.Tensor>. Shape: (). The loss function. If the <grad_fn> attribute is None then there is an issue.\n","    '''\n","\n","    numerical_stabilizer = 10**-20\n","\n","    mask = torch.ones(p_distr_ground_truth.shape, dtype=torch.float32, device=device)\n","    if ignore_index != None:\n","        mask[:,:,ignore_index] = 0\n","\n","    scaler = 400. / mask.sum()\n","    loss_ = - scaler * torch.sum((mask * p_distr_ground_truth) * torch.log(p_distr_pred+numerical_stabilizer), axis=(0, 1, 2))\n","\n","    return loss_\n","\n","def bleu_k(col_pred, col_ground_truth, max_k, unk, eos):\n","    '''\n","    Description:\n","        BLEU function defined with respect to 1-grams, ..., max_k-grams. <d0> is defined to either be <int> or <str>. The inputs have to be trimmed and as a result they shouldn't contain the end of sequence or padding tokens.\n","\n","    Examples:\n","        bleu = bleu_k(col_pred=pred_str, col_ground_truth=tgt_str, max_k=2, unk='<unk>', eos='<eos>')\n","\n","    Inputs:\n","        <col_pred>: Type: <list[<list[<d0>]>]>. The outer list's length is n_examples. The inner list's length can vary depending on the corresponding number of steps.\n","        <col_ground_truth>: Type: <list[<list[<d0>]>]>. The outer list's length is n_examples. The inner list's length can vary depending on the corresponding number of steps.\n","        <max_k>: Type: <int>. Maximum number of gram mergings.\n","        <unk>: Type: <d0>. The unknown token in integer format.\n","        <eos>: Type: <d0>. The end-of-sequence token in integer format.\n","\n","    Outputs:\n","        <bleu_k_value>: Type <float>.\n","    '''\n","\n","    assert len(col_pred) == len(col_ground_truth), 'E: Input collections do not share a common multitude of examples.'\n","\n","    n_examples = len(col_pred)\n","\n","    ## Trimming\n","    col_pred = trim_sequence(seq=col_pred, eos=eos)\n","    col_ground_truth = trim_sequence(seq=col_ground_truth, eos=eos)\n","\n","    ## Make the unkown token differ in the sequences to prevent it from contributing to the score. There are, of course, many ways to achieve this.\n","    col_ground_truth = [[token if (token != unk) else -1 for token in example] for example in col_ground_truth]\n","\n","    col_pred_kgrams = []\n","    col_ground_truth_kgrams = []\n","\n","    bleu_ = [None for i in range(n_examples)]\n","\n","    for k_idx in range(max_k):\n","        k = k_idx+1\n","\n","        col_pred_kgrams.append(single_gram_to_k_grams(col_pred, k))\n","        col_ground_truth_kgrams.append(single_gram_to_k_grams(col_ground_truth, k))\n","\n","    for i in range(n_examples):\n","\n","        product_of_weighted_precisions = 1\n","        n_pred = len(col_pred[i])\n","        n_ground_truth = len(col_ground_truth[i])\n","\n","        if min(n_pred, n_ground_truth) == 0:\n","            if n_ground_truth == 0:\n","                bleu_[i] = 1\n","            else: ## equivalent to `elif (n_ground_truth == 1) and (n_pred==0):`\n","                bleu_[i] = 0\n","        else:\n","            brevity_penalty = min(1.0, math.exp(1 - n_ground_truth/n_pred))\n","\n","            for k_idx in range(max_k):\n","                k = k_idx+1\n","\n","                kgram_instability = k > min([min(len(col_pred[i]), len(col_ground_truth[i])) for i in range(n_examples)])\n","                if kgram_instability:\n","                    break\n","\n","                kgrams_precision = 0. ## Given an example i, for each member of <valid_kgrams>, compute the number of occurences inside <col_ground_truth_kgrams[k_idx][i]> and inside <col_pred_kgrams[k_idx][i]>, take their minimum and add that value. Finally divide by the number of predicted k-grams. The rest is obvious.\n","\n","                n_kgrams_pred = len(col_pred_kgrams[k_idx][i])\n","                n_kgrams_ground_truth = len(col_ground_truth_kgrams[k_idx][i])\n","\n","                valid_kgrams = set() ## kgrams that exist inside <col_ground_truth_kgrams[k_idx][i]>.\n","\n","                ## Find the common tokens\n","                for seq_pred_idx in range(n_kgrams_pred):\n","                    if col_pred_kgrams[k_idx][i][seq_pred_idx] in col_ground_truth_kgrams[k_idx][i]:\n","                        valid_kgrams.add(col_pred_kgrams[k_idx][i][seq_pred_idx])\n","\n","                for valid_kgram in valid_kgrams:\n","                    n_occurences_pred = col_pred_kgrams[k_idx][i].count(valid_kgram)\n","                    n_occurences_ground_truth = col_ground_truth_kgrams[k_idx][i].count(valid_kgram)\n","                    kgrams_precision += min((n_occurences_pred, n_occurences_ground_truth))\n","\n","                kgrams_precision /= n_kgrams_pred\n","\n","                product_of_weighted_precisions *= kgrams_precision**((1/2)**k)\n","\n","            bleu_[i] = brevity_penalty * product_of_weighted_precisions\n","\n","    bleu = sum(bleu_)/n_examples\n","\n","    return bleu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ERAxnSXy8p5"},"outputs":[],"source":["class figure:\n","\n","    def __init__(self, metric_names, pipeline_names):\n","\n","        self.metric_names = metric_names\n","        self.pipeline_names = pipeline_names\n","\n","        ## 2 side by side axes\n","        asp_ratio = 0.7# 0.4375 \n","        l = 1400# 1400\n","        h = asp_ratio * l\n","        self.fig = plt.figure(figsize=(l/96, h/96), dpi=96)\n","        mpl_style(dark=True)\n","\n","        self.ax = \\\n","        [\n","            [\n","                None\n","                for metric_idx in range(len(self.metric_names))\n","            ] for pipeline_idx in range(len(self.pipeline_names))\n","        ]\n","\n","        ax_idx = 0\n","        for pipeline_idx in range(len(self.pipeline_names)):\n","            for metric_idx in range(len(self.metric_names)):\n","                ax_idx += 1\n","                self.ax[pipeline_idx][metric_idx] = self.fig.add_subplot\\\n","                (\n","                    len(self.pipeline_names)*100 + len(self.metric_names)*10 + ax_idx\n","                )\n","\n","    def plot(self, hor_seq, metrics_history):\n","\n","        def plot_(hor_seq, ax_idx, train_seq, val_seq, ver_name, title_name):\n","\n","            pipeline_idx, metric_idx = ax_idx\n","\n","            ## Clearing previous frame\n","            self.ax[metric_idx][pipeline_idx].clear()\n","\n","            ## To show only integer numbers on the x-axis\n","            self.ax[metric_idx][pipeline_idx].xaxis.set_major_locator(MaxNLocator(integer=True))\n","\n","            self.ax[metric_idx][pipeline_idx].set_xlabel('epoch')\n","            self.ax[metric_idx][pipeline_idx].set_ylabel(ver_name)\n","\n","            self.ax[metric_idx][pipeline_idx].plot(hor_seq, train_seq, color='red', label='etr %s'%(ver_name))\n","            self.ax[metric_idx][pipeline_idx].plot(hor_seq, val_seq, color='cyan', label='val %s'%(ver_name))\n","\n","            self.ax[metric_idx][pipeline_idx].set_title(title_name)\n","\n","            self.ax[metric_idx][pipeline_idx].legend()\n","\n","            # self.ax[metric_idx][pipeline_idx].grid(visible=True, color='gray', alpha=0.5)\n","\n","        for (pipeline_idx, pipeline_name) in enumerate(self.pipeline_names):\n","            pipeline_name_ = deepcopy(pipeline_name)\n","            for (metric_idx, metric_name) in enumerate(self.metric_names):\n","                plot_\\\n","                (\n","                    hor_seq=hor_seq,\n","                    ax_idx=(pipeline_idx, metric_idx),\n","                    train_seq=metrics_history[metric_name][pipeline_name]['train'],\n","                    val_seq=metrics_history[metric_name][pipeline_name]['val'],\n","                    ver_name=self.metric_names[metric_idx],\n","                    title_name=pipeline_name_,\n","                )\n","                pipeline_name_ = ''\n","\n","\n","def plot_sentence_size(data_pair, image_name):\n","\n","    mpl_style(dark=True)\n","\n","    src, tgt = data_pair\n","\n","    src_sentence_len = [len(sentence) for sentence in src]\n","    tgt_sentence_len = [len(sentence) for sentence in tgt]\n","\n","    src_sentence_len_counted = Counter(src_sentence_len)\n","    src_sentence_len_counted_sorted = sorted(src_sentence_len_counted.items(), key=lambda x: x[0], reverse=False)\n","\n","    tgt_sentence_len_counted = Counter(tgt_sentence_len)\n","    tgt_sentence_len_counted_sorted = sorted(tgt_sentence_len_counted.items(), key=lambda x: x[0], reverse=False)\n","\n","    src_possible_sentence_len = [element[0] for element in src_sentence_len_counted_sorted]\n","    src_max_possible_sentence_len = max(src_possible_sentence_len)\n","\n","    tgt_possible_sentence_len = [element[0] for element in tgt_sentence_len_counted_sorted]\n","    tgt_max_possible_sentence_len = max(tgt_possible_sentence_len)\n","\n","    max_possible_sentence_len = max([src_max_possible_sentence_len, tgt_max_possible_sentence_len])\n","\n","    range_possible_sentence_len = list(range(0, max_possible_sentence_len+1))\n","    src_sentence_len_counted_sorted_extended = [0 for sentence_len in range_possible_sentence_len]\n","    tgt_sentence_len_counted_sorted_extended = [0 for sentence_len in range_possible_sentence_len]\n","\n","    for sentence_len in range_possible_sentence_len:\n","\n","        if sentence_len in src_sentence_len_counted.keys():\n","            src_sentence_len_counted_sorted_extended[sentence_len] = src_sentence_len_counted[sentence_len]\n","\n","        if sentence_len in tgt_sentence_len_counted.keys():\n","            tgt_sentence_len_counted_sorted_extended[sentence_len] = tgt_sentence_len_counted[sentence_len]\n","\n","    asp_ratio = 0.8\n","    l = 700\n","    h = asp_ratio * l\n","    plt.figure(figsize=(l/96, h/96), dpi=96)\n","\n","    plt.plot(range_possible_sentence_len, src_sentence_len_counted_sorted_extended, color='orange', label='source')\n","    plt.plot(range_possible_sentence_len, tgt_sentence_len_counted_sorted_extended, color='lime', label='target')\n","    # plt.title('Frequency Graph')\n","    plt.xticks(range_possible_sentence_len[::5])\n","    plt.xlabel('Token Count')\n","    plt.ylabel('# of Instances')\n","    plt.legend()\n","    plt.grid(visible=True, color='gray', alpha=0.5)\n","\n","    plt.savefig('./drive/MyDrive/colab_storage/datasets/'+image_name)\n","\n","def plot_frequency_curves(freqs_pair, image_name):\n","\n","    mpl_style(dark=True)\n","\n","    asp_ratio = 0.8\n","    l = 700\n","    h = asp_ratio * l\n","    plt.figure(figsize=(l/96, h/96), dpi=96)\n","\n","    src_freqs, tgt_freqs = freqs_pair\n","\n","    src_word_keys = list(range(len(src_freqs)))\n","    plt.plot(src_word_keys, src_freqs, color='orange', label='source')\n","    tgt_word_keys = list(range(len(tgt_freqs)))\n","    plt.plot(tgt_word_keys, tgt_freqs, color='lime', label='target')\n","\n","    # plt.title('Frequency Graph')\n","    plt.xlabel('Token Index')\n","    plt.ylabel('Frequency')\n","    plt.grid(visible=True, color='gray', alpha=0.5)\n","    plt.semilogx()\n","    plt.semilogy()\n","\n","    plt.legend()\n","\n","    plt.savefig('./drive/MyDrive/colab_storage/datasets/'+image_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cg8WwCuXy8p6"},"outputs":[],"source":["class rnn_trainer(nn.Module):\n","\n","    def __init__(self, model_, data, device):\n","        \"\"\"\n","        Inputs:\n","            <model_>: Type: <torch.nn.module>. Specifies the trained model's architecture and parameters.\n","            <data>: Type: <class>. Contains all the necessary dataset's information.\n","            <device>: Type: <torch.device>.\n","        \"\"\"\n","\n","        super().__init__()\n","\n","        self.device = device\n","\n","        self.model = model_\n","        self.data = data\n","\n","        self.epochs = 1000\n","        self.lr = 0.005\n","        self.minibatch_size = 2**10\n","\n","        # self.decay_rate = 16.666 # DEFAULT: 16.666\n","        # self.min_decay = 5\n","        # self.decay_scheduler = lambda epoch: max(self.min_decay, self.decay_rate/(self.decay_rate+math.exp(epoch/self.decay_rate)))\n","        self.decay_scheduler = lambda epoch: 9/9\n","\n","        ## Backup frequence used for live training monitoring\n","        self.bkp_freq = 10\n","\n","        ## Backup frequency of saved results\n","        saved_bkp_freq = 10\n","        self.scheduled_checkpoints = {}#{10, 20, 30, 60, 90, 120, 150, 200, 250, 300, 350, 400, 500, 600, 700, 800, 1000} #{saved_bkp_freq*ep for ep in range(self.epochs // saved_bkp_freq)}; self.scheduled_checkpoints.remove(0)\n","\n","        self.metric_names = ['loss', 'bleu']\n","        self.pipeline_names = ['training_pipeline', 'id2_pipeline']\n","        self.data_subset_names = ['train', 'val']\n","\n","        self.training_dir_path, self.training_format = './drive/MyDrive/colab_storage/training/', '.pt'\n","        self.criterion = categ_cross_entropy ## Has to be masked\n","        self.optimizer = torch.optim.Adam(params=self.model.parameters(), lr=self.lr)\n","\n","        self.train_dataloader = torch.utils.data.DataLoader(self.data.train_set, batch_size=self.minibatch_size, shuffle=True)\n","        self.val_dataloader = torch.utils.data.DataLoader(self.data.val_set, batch_size=2**10, shuffle=False)\n","\n","        ## One hot encoder of tensor <tsr> having shape (n_examples, n_steps, n) and number of classes <n_classes>.\n","        self.ohe_modifier = lambda tsr, n_classes: torch.stack([F.one_hot(tsr[:,seq_idx], n_classes) for seq_idx in range(tsr.shape[-1])], axis=0).swapaxes(0, 1).type(torch.float32).to(self.device)\n","\n","        n_examples_display = 5\n","        X_train_small_idcs = torch.round(torch.linspace(51, self.data.n_train-1, n_examples_display)).type(torch.int)\n","        self.X_train_small = self.ohe_modifier(self.data.X_train[X_train_small_idcs.to(torch.long)], self.data.src_vocab_size)\n","        X_val_small_idcs = torch.round(torch.linspace(51, self.data.n_val-1, n_examples_display)).type(torch.int)\n","        self.X_val_small = self.ohe_modifier(self.data.X_val[X_val_small_idcs.to(torch.long)], self.data.src_vocab_size)\n","\n","    def train(self, metrics_history=None):\n","        \"\"\"\n","        Description:\n","            Trains a model.\n","\n","        Inputs:\n","            <metrics_history>: Type: <dict>. Holds all training's history evaluation measurements.\n","        \"\"\"\n","\n","        def print_metrics_per_pipeline(pipeline_name):\n","            print('> '+pipeline_name)\n","            print('>> '+'Train loss: %f | Val loss: %f'%(metrics_history['loss'][pipeline_name]['train'][-1], metrics_history['loss'][pipeline_name]['val'][-1]))\n","            print('>> '+'Train BLEU: %f | Val BLEU: %f'%(metrics_history['bleu'][pipeline_name]['train'][-1], metrics_history['bleu'][pipeline_name]['val'][-1]))\n","\n","        def get_metrics(prediction, ground_truth):\n","\n","            loss = self.criterion\\\n","            (\n","                p_distr_pred=prediction,\n","                p_distr_ground_truth=ground_truth,\n","                device=self.device,\n","                ignore_index=self.data.tgt_vocab.get_stoi()['<pad>']\n","            )\n","            bleu = torch.tensor\\\n","            (\n","                bleu_k\\\n","                (\n","                    col_pred=ohe2int(prediction).tolist(), \n","                    col_ground_truth=ohe2int(ground_truth).tolist(),\n","                    max_k=2,\n","                    unk=self.data.tgt_vocab.get_stoi()['<unk>'],\n","                    eos=self.data.tgt_vocab.get_stoi()['<eos>']\n","                )\n","            ).to(device=self.device).detach()\n","\n","            return loss, bleu\n","\n","        if metrics_history == None:\n","            metrics_history = \\\n","            {\n","                metric_name:\n","                {\n","                    pipeline_name:\n","                    {\n","                        data_subset: [] for data_subset in self.data_subset_names\n","                    } for pipeline_name in self.pipeline_names\n","                } for metric_name in self.metric_names\n","            }\n","\n","        print('Number of training instances: %d'%(self.data.n_train))\n","        print('Training Status:')\n","\n","        if metrics_history['bleu']['id2_pipeline']['val'] == []:\n","            epochs_bleu_epoch_id2_pipeline_val_hat_prev = float('inf')\n","        else:\n","            epochs_bleu_epoch_id2_pipeline_val_hat_prev = metrics_history['bleu']['id2_pipeline']['val'][-1]\n","\n","        figure_ = figure(metric_names=self.metric_names, pipeline_names=self.pipeline_names)\n","        initial_epoch = len(metrics_history['loss']['training_pipeline']['val'])\n","        t_before_training = time()\n","        for epoch in range(initial_epoch, self.epochs):\n","            t_i = time()\n","\n","            losses_minibatch_training_pipeline_train = []\n","            losses_minibatch_training_pipeline_val = []\n","            losses_minibatch_id2_pipeline_train = []\n","            losses_minibatch_id2_pipeline_val = []\n","\n","            bleus_minibatch_training_pipeline_train = []\n","            bleus_minibatch_training_pipeline_val = []\n","            bleus_minibatch_id2_pipeline_train = []\n","            bleus_minibatch_id2_pipeline_val = []\n","\n","            for (i, instance_i) in enumerate(self.train_dataloader):\n","\n","                ## ! Training step initialization: Begin\n","\n","                current_parameters_are_optimal = False\n","\n","                X_train_minibatch_i = self.ohe_modifier(instance_i[0], self.data.src_vocab_size)\n","                Y_train_minibatch_i = self.ohe_modifier(instance_i[1], self.data.tgt_vocab_size)\n","                Y_bos_train_minibatch_i = self.ohe_modifier(instance_i[2], self.data.tgt_vocab_size)\n","\n","                ## Resetting gradient tensor variables\n","                self.optimizer.zero_grad()\n","\n","                ## ! Training step initialization: End\n","\n","                ## Forward propagation\n","                prediction_minibatch_training_pipeline_train = self.model(dec_mode=1, dec_config={'decay': self.decay_scheduler(epoch)}, train=True, X=X_train_minibatch_i, Y=Y_bos_train_minibatch_i)\n","                prediction_minibatch_id2_pipeline_train = self.model(dec_mode=2, dec_config=None, train=False, X=X_train_minibatch_i, Y=None)\n","\n","                ## ! Loss and other evaluation metrics: Begin\n","\n","                ## ! Training pipeline: Begin\n","\n","                loss_minibatch_training_pipeline_train, \\\n","                bleu_minibatch_training_pipeline_train \\\n","                = get_metrics\\\n","                (\n","                    prediction=prediction_minibatch_training_pipeline_train, ground_truth=Y_train_minibatch_i\n","                )\n","\n","                losses_minibatch_training_pipeline_train.append(loss_minibatch_training_pipeline_train.detach())\n","                bleus_minibatch_training_pipeline_train.append(bleu_minibatch_training_pipeline_train)\n","\n","                ## ! Training pipeline: End\n","\n","                ## ! Prediction pipeline with ID == 2: Begin\n","\n","                loss_minibatch_id2_pipeline_train, \\\n","                bleu_minibatch_id2_pipeline_train \\\n","                = get_metrics\\\n","                (\n","                    prediction=prediction_minibatch_id2_pipeline_train, ground_truth=Y_train_minibatch_i\n","                )\n","\n","                losses_minibatch_id2_pipeline_train.append(loss_minibatch_id2_pipeline_train.detach())\n","                bleus_minibatch_id2_pipeline_train.append(bleu_minibatch_id2_pipeline_train)\n","\n","                ## ! Prediction pipeline with ID == 2: End\n","\n","                ## ! Loss and other evaluation metrics: End\n","\n","                ## Backpropagation - Gradients computation\n","                loss_minibatch_training_pipeline_train.backward()\n","\n","                self.clip_gradients(grad_clip_val=1, model=self.model)\n","\n","                ## Gradient Descent - Gradient update\n","                self.optimizer.step()\n","\n","            with torch.no_grad():\n","\n","                for (j, instance_j) in enumerate(self.val_dataloader):\n","\n","                    ## ! Validation/Test step initialization: Begin\n","\n","                    X_val_minibatch_j = self.ohe_modifier(instance_j[0], self.data.src_vocab_size)\n","                    Y_val_minibatch_j = self.ohe_modifier(instance_j[1], self.data.tgt_vocab_size)\n","                    Y_bos_val_minibatch_j = self.ohe_modifier(instance_j[2], self.data.tgt_vocab_size)\n","\n","                    ## ! Validation/Test step initialization: End\n","\n","                    ## Forward propagation\n","                    prediction_minibatch_training_pipeline_val = self.model(dec_mode=1, dec_config={'decay': self.decay_scheduler(epoch)}, train=False, X=X_val_minibatch_j, Y=Y_bos_val_minibatch_j)\n","                    prediction_minibatch_id2_pipeline_val = self.model(dec_mode=2, dec_config=None, train=False, X=X_val_minibatch_j, Y=None).detach()\n","\n","                    ## ! Loss and other evaluation metrics: Begin\n","\n","                    ## ! Training pipeline: Begin\n","\n","                    loss_minibatch_training_pipeline_val, \\\n","                    bleu_minibatch_training_pipeline_val \\\n","                    = get_metrics\\\n","                    (\n","                        prediction=prediction_minibatch_training_pipeline_val, ground_truth=Y_val_minibatch_j\n","                    )\n","\n","                    losses_minibatch_training_pipeline_val.append(loss_minibatch_training_pipeline_val)\n","                    bleus_minibatch_training_pipeline_val.append(bleu_minibatch_training_pipeline_val)\n","\n","                    ## ! Training pipeline: End\n","\n","                    ## ! Prediction pipeline with ID == 2: Begin\n","\n","                    loss_minibatch_id2_pipeline_val, \\\n","                    bleu_minibatch_id2_pipeline_val \\\n","                    = get_metrics\\\n","                    (\n","                        prediction=prediction_minibatch_id2_pipeline_val, ground_truth=Y_val_minibatch_j\n","                    )\n","\n","                    losses_minibatch_id2_pipeline_val.append(loss_minibatch_id2_pipeline_val)\n","                    bleus_minibatch_id2_pipeline_val.append(bleu_minibatch_id2_pipeline_val)\n","\n","                    ## ! Prediction pipeline with ID == 2: End\n","\n","                    ## ! Loss and other evaluation metrics: End\n","\n","            ## ! Training pipeline: Begin\n","\n","            loss_epoch_training_pipeline_train_hat = torch.sum(torch.stack(losses_minibatch_training_pipeline_train)) / len(losses_minibatch_training_pipeline_train)\n","            metrics_history['loss']['training_pipeline']['train'].append(loss_epoch_training_pipeline_train_hat.item())\n","            bleu_epoch_training_pipeline_train_hat = torch.sum(torch.stack(bleus_minibatch_training_pipeline_train)) / len(bleus_minibatch_training_pipeline_train)\n","            metrics_history['bleu']['training_pipeline']['train'].append(bleu_epoch_training_pipeline_train_hat.item())\n","\n","            loss_epoch_training_pipeline_val_hat = torch.sum(torch.stack(losses_minibatch_training_pipeline_val)) / len(losses_minibatch_training_pipeline_val)\n","            metrics_history['loss']['training_pipeline']['val'].append(loss_epoch_training_pipeline_val_hat.item())\n","            bleu_epoch_training_pipeline_val_hat = torch.sum(torch.stack(bleus_minibatch_training_pipeline_val)) / len(bleus_minibatch_training_pipeline_val)\n","            metrics_history['bleu']['training_pipeline']['val'].append(bleu_epoch_training_pipeline_val_hat.item())\n","\n","            ## ! Training pipeline: End\n","\n","            ## ! Prediction pipeline with ID == 2: Begin\n","\n","            loss_epoch_id2_pipeline_train_hat = torch.sum(torch.stack(losses_minibatch_id2_pipeline_train)) / len(losses_minibatch_id2_pipeline_train)\n","            metrics_history['loss']['id2_pipeline']['train'].append(loss_epoch_id2_pipeline_train_hat.item())\n","            bleu_epoch_id2_pipeline_train_hat = torch.sum(torch.stack(bleus_minibatch_id2_pipeline_train)) / len(bleus_minibatch_id2_pipeline_train)\n","            metrics_history['bleu']['id2_pipeline']['train'].append(bleu_epoch_id2_pipeline_train_hat.item())\n","\n","            loss_epoch_id2_pipeline_val_hat = torch.sum(torch.stack(losses_minibatch_id2_pipeline_val)) / len(losses_minibatch_id2_pipeline_val)\n","            metrics_history['loss']['id2_pipeline']['val'].append(loss_epoch_id2_pipeline_val_hat.item())\n","            bleu_epoch_id2_pipeline_val_hat = torch.sum(torch.stack(bleus_minibatch_id2_pipeline_val)) / len(bleus_minibatch_id2_pipeline_val)\n","            metrics_history['bleu']['id2_pipeline']['val'].append(bleu_epoch_id2_pipeline_val_hat.item())\n","\n","            ## ! Prediction pipeline with ID == 2: End\n","\n","            figure_.plot\\\n","            (\n","                hor_seq=np.arange(0, epoch+1),\n","                metrics_history = metrics_history\n","            )\n","\n","            current_parameters_are_optimal = bleu_epoch_id2_pipeline_val_hat.item() >= max(metrics_history['bleu']['id2_pipeline']['val'])\n","\n","            self.save_training(epoch, metrics_history=metrics_history, thparams=(self.lr, self.minibatch_size), t_before_training=t_before_training, figure_=figure_, current_parameters_are_optimal=current_parameters_are_optimal)\n","\n","            t_f = time()\n","            delta_t = round(t_f-t_i)\n","            est_next_epoch_time = datetime.datetime.utcfromtimestamp(t_f) + datetime.timedelta(seconds=delta_t)\n","\n","            print('[Epoch %d @ UTC %s]'%(epoch, datetime.datetime.utcfromtimestamp(t_f).strftime(\"%H:%M:%S\")))\n","\n","            print_metrics_per_pipeline(pipeline_name='training_pipeline')\n","            print_metrics_per_pipeline(pipeline_name='id2_pipeline')\n","\n","            print('Δt: %ds | Δ training pipeline\\'s val bleu: %f\\nNext epoch @ ~UTC %s'%(delta_t, metrics_history['bleu']['training_pipeline']['val'][-1]-epochs_bleu_epoch_id2_pipeline_val_hat_prev, est_next_epoch_time.strftime(\"%H:%M:%S\")))\n","\n","            pred_train_small_ohe = self.model(dec_mode=2, dec_config=None, train=False, X=self.X_train_small, Y=None).detach()\n","            pred_train_small_int = ohe2int(pred_train_small_ohe)\n","            pred_train_small_str = int2str(col_int=pred_train_small_int, vocab=self.data.tgt_vocab, max_steps=self.data.max_steps_tgt)\n","            X_train_small_str = ohe2str(self.X_train_small, self.data.src_vocab, self.data.max_steps_src)\n","\n","            pred_val_small_ohe = self.model(dec_mode=2, dec_config=None, train=False, X=self.X_val_small, Y=None).detach()\n","            pred_val_small_int = ohe2int(pred_val_small_ohe)\n","            pred_val_small_str = int2str(col_int=pred_val_small_int, vocab=self.data.tgt_vocab, max_steps=self.data.max_steps_tgt)\n","            X_val_small_str = ohe2str(self.X_val_small, self.data.src_vocab, self.data.max_steps_src)\n","\n","            print('Training set examples:')\n","            for example_idx in range(len(pred_train_small_str)):\n","                print('%-4s'%(str(example_idx)), end='')\n","                print('%-40s   '%(X_train_small_str[example_idx]), end='')\n","                print('%s'%(pred_train_small_str[example_idx]))\n","            print('Validation set examples:')\n","            for example_idx in range(len(pred_val_small_str)):\n","                print('%-4s'%(str(example_idx)), end='')\n","                print('%-40s   '%(X_val_small_str[example_idx]), end='')\n","                print('%s'%(pred_val_small_str[example_idx]))\n","\n","            print()\n","\n","            epochs_bleu_epoch_id2_pipeline_val_hat_prev = deepcopy(metrics_history['bleu']['id2_pipeline']['val'][-1])\n","\n","            torch.cuda.empty_cache()\n","\n","        print('Training completed.')\n","\n","    def clip_gradients(self, grad_clip_val, model):\n","\n","        params = [p for p in model.parameters() if p.requires_grad]\n","        norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n","        if norm > grad_clip_val:\n","            for param in params:\n","                param.grad[:] *= grad_clip_val / norm\n","\n","    def save_training(self, epoch, metrics_history, thparams, t_before_training, figure_, current_parameters_are_optimal):\n","\n","        def get_training_information(epoch, metrics_history, thparams, t_before_training):\n","\n","            lr, minibatch_size = thparams\n","\n","            training_information = \\\n","            {\n","                'model_params': self.model.state_dict(),\n","                'src_vocab': self.data.src_vocab,\n","                'src_vocab_size': self.data.src_vocab_size,\n","                'tgt_vocab': self.data.tgt_vocab,\n","                'tgt_vocab_size': self.data.tgt_vocab_size,\n","                'max_steps_src': self.data.max_steps_src,\n","                'max_steps_tgt': self.data.max_steps_tgt,\n","                'data_info':\n","                {\n","                    'n_train': self.data.n_train,\n","                    'n_val': self.data.n_val\n","                },\n","                'metrics_history': metrics_history,\n","                'training_hparams':\n","                {\n","                    'epoch': epoch,\n","                    'learning_rate': lr,\n","                    'minibatch_size': minibatch_size\n","                },\n","                'delta_t': time()-t_before_training,\n","                'shuffle_seed': self.data.shuffle_seed,\n","                'dataset_name': self.data.dataset_name\n","            }\n","\n","            return training_information\n","\n","        ## Scheduled backup - model thread\n","        if epoch in self.scheduled_checkpoints:\n","            training_information = get_training_information(epoch, metrics_history, thparams, t_before_training)\n","            training_scheduled_backup_path = self.training_dir_path + self.model.name + '_ep' + str(epoch) + self.training_format\n","            torch.save(training_information, training_scheduled_backup_path)\n","\n","        ## Latest frequent backups\n","        if (epoch != 0) and ((epoch % self.bkp_freq) == 0):\n","            training_information = get_training_information(epoch, metrics_history, thparams, t_before_training)\n","            live_training_backup_path = self.training_dir_path + self.model.name + '_live_ep' + str(epoch) + self.training_format\n","            prev_live_training_backup_path = self.training_dir_path + self.model.name + '_live_ep' + str(epoch-self.bkp_freq) + self.training_format\n","            if os.path.exists(prev_live_training_backup_path):\n","                os.remove(prev_live_training_backup_path)\n","            torch.save(training_information, live_training_backup_path)\n","\n","        if current_parameters_are_optimal:\n","            training_information = get_training_information(epoch, metrics_history, thparams, t_before_training)\n","            training_opt_backup_path = self.training_dir_path + self.model.name + '_opt' + self.training_format\n","            if os.path.exists(training_opt_backup_path):\n","                os.remove(training_opt_backup_path)\n","            torch.save(training_information, training_opt_backup_path)\n","\n","        figure_.fig.savefig(self.training_dir_path + self.model.name + '_live')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cXAjSbLny8p-"},"outputs":[],"source":["def trivialize_irrelevant_possibilities(p_distr, irrelevant_possibility_indices):\n","    '''\n","    Description:\n","        The values corresponding to indices of <p_distr> that match the index-values specified by <irrelevant_possibility_indices> are substituted by 0. The removed probability quantity is added up and distributed equally to the rest of the probabilities. This is identical to the action of shrinking the sample space i.e. finding a *conditional* random variable.\n","\n","    Inputs:\n","        <p_distr>: Type: <torch.Tensor>. Shape: (..., n_possibilities).\n","        <irrelevant_possibility_indices>: Type: <list[<int>]>.\n","\n","    Outputs:\n","        <updated_p_distr>: Type: <torch.Tensor>. Shape: (..., n_possibilities).\n","    '''\n","\n","    updated_p_distr = deepcopy(p_distr)\n","\n","    n_possibilities = updated_p_distr.shape[-1]\n","    n_relevant_possibilities = n_possibilities - len(irrelevant_possibility_indices)\n","\n","    redistributable_probability = torch.zeros((p_distr.shape[0], p_distr.shape[1]), device=p_distr.device)\n","    for irrelevant_possibility in irrelevant_possibility_indices:\n","        redistributable_probability += p_distr[..., irrelevant_possibility]\n","        updated_p_distr[..., irrelevant_possibility] = 0\n","\n","    redistributable_probability /= n_relevant_possibilities\n","\n","    for possibility in range(n_possibilities):\n","        if possibility not in irrelevant_possibility_indices:\n","            relevant_possibility = possibility\n","            updated_p_distr[..., relevant_possibility] += redistributable_probability\n","\n","    return updated_p_distr\n","\n","class predictor:\n","\n","    def __init__(self, model, src_vocab, src_vocab_size, max_steps_src, tgt_vocab, tgt_vocab_size, max_steps_tgt, tgt_name, device):\n","\n","        self.device = device\n","\n","        self.model = model\n","\n","        self.beam_width = 2\n","\n","        self.src_vocab = src_vocab\n","        self.src_vocab_size = src_vocab_size\n","        self.max_steps_src = max_steps_src\n","\n","        self.tgt_vocab = tgt_vocab\n","        self.tgt_vocab_size = tgt_vocab_size\n","        self.max_steps_tgt = max_steps_tgt\n","\n","        self.tgt_name = tgt_name\n","\n","        ## Source text preprocess functions\n","        self.initial_text_preprocess = initial_text_preprocess\n","        self.tokenizer = lambda x: [[t for t in f'{x} <eos>'.split(' ') if t]]\n","        self.src_numericalize = functools.partial(c_numericalize_str, vocab_itos=self.src_vocab.get_itos())\n","        self.src_c_pad_or_trim = functools.partial(c_pad_or_trim, eos_int=self.src_vocab.get_stoi()['<eos>'], pad_int=self.src_vocab.get_stoi()['<pad>'], t_bound=self.max_steps_src)\n","        self.src_ohe_modifier = lambda x: torch.stack([F.one_hot(x[seq_idx], self.src_vocab_size) for seq_idx in range(len(x))], axis=0).type(torch.float32).to(self.device)\n","\n","    def __call__(self, x, dec_mode, dec_config):\n","        '''\n","        Inputs:\n","            <x>: Contains the input's information. Can be a list of strings, or a torch,tensor where its contents' shared data type may be integer or float.\n","            <dec_mode>: Type: <int>. Decoder's pipeline ID.\n","            <dec_config>: Type: <dict>. Decoder's pipeline configuration.\n","        '''\n","\n","        self.dec_mode = dec_mode\n","\n","        if isinstance(x[0], str):\n","            self.src_str = x\n","            self.src_ohe = self.preprocess(self.src_str).to(self.device)\n","        elif isinstance(x[0,0].item(), (int, float)):\n","            self.src_str = int2str(col_int=x, vocab=self.src_vocab, max_steps=self.max_steps_src)\n","            self.src_ohe = self.src_ohe_modifier(x)\n","\n","        with torch.no_grad():\n","            self.p_distr = self.model(X=self.src_ohe, dec_mode=self.dec_mode, dec_config=dec_config, train=False)\n","\n","        self.n_examples = len(self.p_distr)\n","\n","        tgt_remove_tokens_str = ['<unk>', '<pad>', '<bos>']\n","        tgt_remove_tokens_int = [self.tgt_vocab.get_stoi()[token] for token in tgt_remove_tokens_str]\n","        self.adjusted_p_distr = trivialize_irrelevant_possibilities(self.p_distr, tgt_remove_tokens_int)\n","\n","        ## Deterministic translation\n","        self.translated_int = ohe2int(self.adjusted_p_distr)\n","        self.translated_str = int2str(col_int=self.translated_int, vocab=self.tgt_vocab, max_steps=self.max_steps_tgt)\n","\n","    def preprocess(self, x):\n","\n","        preprocessed_x = [None for i in range(len(x))]\n","        for i in range(len(x)):\n","            preprocessed_x[i] = self.initial_text_preprocess(x[i])\n","            preprocessed_x[i] = self.tokenizer(preprocessed_x[i])\n","            preprocessed_x[i] = self.src_numericalize(preprocessed_x[i])\n","            preprocessed_x[i] = self.src_c_pad_or_trim(preprocessed_x[i])\n","            preprocessed_x[i] = torch.tensor(preprocessed_x[i])\n","            preprocessed_x[i] = self.src_ohe_modifier(preprocessed_x[i])[0]\n","        preprocessed_x = torch.stack(preprocessed_x)\n","\n","        return preprocessed_x\n","\n","    def display_translation(self):\n","\n","        if self.n_examples == 1:\n","            print('Source sequence [eng]:\\n%s'%(self.src_str[0]))\n","            print('Target sequence [%s]:\\n%s'%(self.tgt_name, self.translated_str[0]))\n","        else:\n","            print(' '*4+'%-40s   %s'%('[eng]', '['+self.tgt_name+']'))\n","            for example_idx in range(self.n_examples):\n","                print('%-4s'%(str(example_idx)), end='')\n","                print('%-40s   '%(self.src_str[example_idx]), end='')\n","                print('%s'%(self.translated_str[example_idx]))\n","\n","    def display_n_likely(self, n=None):\n","        '''\n","        Description:\n","            The probability value displayed for a given example i and step t, for the corresponding token x_{i,t} is actually\n","            P(x_{i,t} | x_{i,t-1}, ..., x_{i,0}).\n","        '''\n","\n","        if (self.dec_mode == 4) and ((n == None) or (n > self.beam_width)):\n","            n = self.beam_width\n","\n","        predicted_words_distr, predicted_words_int = torch.topk(input=self.p_distr, k=n, dim=-1) # val, ind lists of shape (self.max_steps_tgt, n) each\n","        predicted_words_str = \\\n","        [\n","            [\n","                [\n","                    self.tgt_vocab.get_itos()[predicted_words_int[example_idx, step, possibility]]\n","                    for possibility in range(n)\n","                ]\n","                for step in range(self.max_steps_tgt)\n","            ]\n","            for example_idx in range(predicted_words_int.shape[0])\n","        ]\n","        predicted_words_distr = predicted_words_distr.tolist()\n","\n","        for example_idx in range(len(predicted_words_str)):\n","            print('Source [eng]:')\n","            print(self.src_str[example_idx])\n","            print('Target token possibilities [%s]:'%(self.tgt_name))\n","            for step in range(self.max_steps_tgt):\n","                print('tkn%d: '%(step), end='')\n","                for possibility in range(n):\n","                    print('%15s: %.4f'%(predicted_words_str[example_idx][step][possibility], predicted_words_distr[example_idx][step][possibility]), end='')\n","                    if possibility != n-1:\n","                        print(' |', end='')\n","                if (example_idx != len(predicted_words_str)-1) or (step != self.max_steps_tgt-1):\n","                    print()\n","            if example_idx != len(predicted_words_str)-1:\n","                print()\n","        print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8RmwL0sYy8p_"},"outputs":[],"source":["def train_from_scratch(device):\n","\n","    data = translated_text_dataset(shuffle_seed=5000)\n","    data.generate_dataset()\n","\n","    print('Source vocabulary size: %d'%(data.src_vocab_size))\n","    print('Target vocabulary size: %d'%(data.tgt_vocab_size))\n","\n","    s2s = seq2seq\\\n","    (\n","        n_src_inp=data.src_vocab_size,\n","        max_steps_src=data.max_steps_src,\n","        n_tgt_inp=data.tgt_vocab_size,\n","        max_steps_tgt=data.max_steps_tgt,\n","        bos_int=data.tgt_vocab.get_stoi()['<bos>'],\n","        eos_int=data.tgt_vocab.get_stoi()['<eos>'],\n","        pad_int=data.tgt_vocab.get_stoi()['<pad>'],\n","        device=device\n","    )\n","    print(s2s)\n","\n","    trainer_ = rnn_trainer(model_=s2s, data=data, device=device)\n","    trainer_.train()\n","\n","def load_and_train_model(training_path, device):\n","\n","    training = torch.load(training_path, map_location=device)\n","\n","    data = translated_text_dataset(shuffle_seed=training['shuffle_seed'])\n","    data.generate_dataset()\n","\n","    print('Source vocabulary size: %d'%(data.src_vocab_size))\n","    print('Target vocabulary size: %d'%(data.tgt_vocab_size))\n","\n","    s2s = seq2seq\\\n","    (\n","        n_src_inp=data.src_vocab_size,\n","        max_steps_src=data.max_steps_src,\n","        n_tgt_inp=data.tgt_vocab_size,\n","        max_steps_tgt=data.max_steps_tgt,\n","        bos_int=data.tgt_vocab.get_stoi()['<bos>'],\n","        eos_int=data.tgt_vocab.get_stoi()['<eos>'],\n","        pad_int=data.tgt_vocab.get_stoi()['<pad>'],\n","        device=device\n","    )\n","    s2s.load_state_dict(training['model_params'])\n","    rnn_trainer_ = rnn_trainer(model_=s2s, data=data, device=device)\n","    rnn_trainer_.train(metrics_history=training['metrics_history'])\n","\n","\n","device = torch.device('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3405385,"status":"error","timestamp":1679490155899,"user":{"displayName":"Apostolos Dimoulakis","userId":"17777759558514461466"},"user_tz":-120},"id":"gkuemWn7w2NU","outputId":"324a3a6b-eef1-4224-f902-a036946ba2d6"},"outputs":[],"source":["train_from_scratch(device=device)\n","# load_and_train_model(training_path='./drive/MyDrive/colab_storage/training/seq2seq_opt.pt', device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_VVCc6Qy8qA"},"outputs":[],"source":["def print_metrics_per_pipeline(pipeline_name, metrics):\n","    print('> '+pipeline_name)\n","    print('>> '+'Train loss: %f | Val loss: %f'%(metrics['loss'][pipeline_name]['train'][-1], metrics['loss'][pipeline_name]['val'][-1]))\n","    print('>> '+'Train BLEU: %f | Val BLEU: %f'%(metrics['bleu'][pipeline_name]['train'][-1], metrics['bleu'][pipeline_name]['val'][-1]))\n","\n","def user_input_loop(translator):\n","\n","    while True:\n","\n","        try:\n","            x = [input('Give a text:\\n')]#['Hello world!', 'What\\'s up?']\n","\n","            translator(x=x, dec_mode=2, dec_config={})\n","\n","            translator.display_translation()\n","            print()\n","            translator.display_n_likely(n=3)\n","            print()\n","        except KeyboardInterrupt: ## Ctrl+C triggers it\n","            exit('\\nProcess terminated.')\n","\n","        exit()\n","\n","def data_sample_evaluation(translator):\n","\n","    data_fpath='../datasets/pp_ell.pt'\n","    n_train = n_val = 5\n","\n","    dataset = torch.load(f=data_fpath)\n","    train_src_int = dataset['train'][:][0][:n_train]\n","    train_tgt_int = dataset['train'][:][1][:n_train]\n","    val_src_int = dataset['val'][:][0][:n_val]\n","    val_tgt_int = dataset['val'][:][1][:n_val]\n","\n","    print('\\nSample from the training set:', end=2*'\\n')\n","    translator(x=train_src_int, dec_mode=2, dec_config={})\n","    translator.display_translation()\n","    # print()\n","    # translator.display_n_likely(n=3)\n","    print(end=2*'\\n')\n","    print('Sample from the validation set:', end=2*'\\n')\n","    translator(x=val_src_int, dec_mode=2, dec_config={})\n","    translator.display_translation()\n","    # print()\n","    # translator.display_n_likely(n=3)\n","\n","\n","device = torch.device('cuda')\n","\n","## The file containing a model\n","training_path = '../training/ell/s2s_ell_ep80.pt'\n","\n","training = torch.load(training_path, map_location=device)\n","\n","print('Source vocabulary size: %d'%(training['src_vocab_size']))\n","print('Target vocabulary size: %d'%(training['tgt_vocab_size']), end=2*'\\n')\n","\n","print('Current epoch is %d.'%(len(training['metrics_history']['loss']['training_pipeline']['train'])-1))\n","print('Translator\\'s evaluation:')\n","print_metrics_per_pipeline(pipeline_name='training_pipeline', metrics=training['metrics_history'])\n","print_metrics_per_pipeline(pipeline_name='id2_pipeline', metrics=training['metrics_history'])\n","\n","s2s = seq2seq\\\n","(\n","    n_src_inp=training['src_vocab_size'],\n","    max_steps_src=training['max_steps_src'],\n","    n_tgt_inp=training['tgt_vocab_size'],\n","    max_steps_tgt=training['max_steps_tgt'],\n","    bos_int=training['tgt_vocab'].get_stoi()['<bos>'],\n","    eos_int=training['tgt_vocab'].get_stoi()['<eos>'],\n","    pad_int=training['tgt_vocab'].get_stoi()['<pad>'],\n","    device=device\n",")\n","s2s.load_state_dict(training['model_params'])\n","\n","translator = predictor.predictor\\\n","(\n","    model=s2s,\n","    src_vocab=training['src_vocab'],\n","    src_vocab_size=training['src_vocab_size'],\n","    max_steps_src=training['max_steps_src'],\n","    tgt_vocab=training['tgt_vocab'],\n","    tgt_vocab_size=training['tgt_vocab_size'],\n","    max_steps_tgt=training['max_steps_tgt'],\n","    tgt_name=training['dataset_name'],\n","    device=device\n",")\n","\n","# user_input_loop(translator=translator)\n","data_sample_evaluation(translator=translator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K8c2FUAS3tAL"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
